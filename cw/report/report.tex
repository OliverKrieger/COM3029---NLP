\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{multicol}
\usepackage{amssymb}
\usepackage[table]{xcolor}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{graphicx}
\graphicspath{ {./imgs/} }

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

\title{
    \huge COM3029 - Natural Language Processing \\
    \Large Individual Coursework \\
    Group 6}
\author{
    Oliver Krieger \\
    URN:6664919
}
\date{April 2024}

\begin{document}

\maketitle

\newpage

\tableofcontents

\newpage

\section{Introduction}

\subsection{Group 6 Members}

\begin{multicols}{2}
\normalsize \begin{enumerate}
  \item Eleanor Lurgio
  \item Oliver Krieger
  \item Seo(Seoeun) Lee
  \item Thiri(Alice) Thu
\end{enumerate}
\end{multicols}

\subsection{Individual Tasks}

\begin{tabular}{ |p{5.1cm}||p{1.2cm}|p{1.1cm}|p{1.2cm}|p{1.1cm}| }
    \hline
    \multicolumn{5}{|c|}{Individual Tasks} \\
    \hline
    Tasks & Eleanor & Oliver & Seoeun & Alice \\
    \hline
    Data Pre-processing &  & \cellcolor{green!25}\checkmark & \cellcolor{green!25}\checkmark &  \\
    \hline
    NLP Algorithms &  &  & \cellcolor{green!25}\checkmark & \cellcolor{green!25}\checkmark \\
    \hline
    Text Encoding / Transformation &  & \cellcolor{green!25}\checkmark & \cellcolor{green!25}\checkmark &  \\
    \hline
    Dataset Splitting & \cellcolor{green!25}\checkmark &  &  & \cellcolor{green!25}\checkmark \\
    \hline
    Loss Functions and Optimisers & \cellcolor{green!25}\checkmark &  &  & \cellcolor{green!25}\checkmark \\
    \hline
    Hyper Parameters & \cellcolor{green!25}\checkmark & \cellcolor{green!25}\checkmark & \cellcolor{green!25}\checkmark &  \\
    \hline
    Pre-trained models & \cellcolor{green!25}\checkmark & \cellcolor{green!25}\checkmark &  & \cellcolor{green!25}\checkmark \\
    \hline
\end{tabular} \\ \\

{\noindent \large \bf My tasks include:}
\begin{enumerate}
  \item Data Pre-processing
  \item Text Encoding / Transformation
  \item Hyper Parameters (Fine Tuning)
  \item Pre-trained models (DistilBERT and SpaCy)
\end{enumerate}

\section{Data Analysis}

\subsection{Loading The Dataset}

For this coursework, we are exploring the PLOD Dataset which is an English-language dataset of abbreviations and their long-forms tagged in text. The data is gathered from research for PLOS journals. The dataset is loaded using the hugging face datasets module. This is done within jupyter notebook environment.

\begin{lstlisting}[language=Python, caption=Load Dataset]
# Import dataset import function for hugging face
from datasets import load_dataset, DatasetDict, Dataset

# import the coursework dataset from
dataset_dict:DatasetDict = load_dataset("surrey-nlp/PLOD-CW") 
\end{lstlisting}
 
This creates a dataset dictionary with 3 datasets - train, validation and test. Each dictionary item has dataset definitions of "tokens", "pos tags" and "ner tags". These are represented as lists of lists of strings (list[list[str]]), the sublists being a representation of rows.

\begin{lstlisting}[language=Python, caption=Dataset Features]
DatasetDict({
    train: Dataset({
        features: ['tokens', 'pos_tags', 'ner_tags'],
        num_rows: 1072
    })
    validation: Dataset({
        features: ['tokens', 'pos_tags', 'ner_tags'],
        num_rows: 126
    })
    test: Dataset({
        features: ['tokens', 'pos_tags', 'ner_tags'],
        num_rows: 153
    })
})
\end{lstlisting}

Each of these datasets can be accessed as normal python dictionaries by calling the dataset name followed by the features name. For instance:
\begin{verbatim}
    dataset["train"]["tokens"]
\end{verbatim}

Would return the list of tokens from the train dataset. As the arrays can become slow to iterate, it was sensible to load them into data collections where that contain data items. The collections allow for easy access to lists and reusable functionality.

\subsection{Data Samples}

After loading the dataset, we can test to see how many samples we have to train on, as well as what our split between labels are. To begin observing this, we will want to write some helper functions to make it easier to read the data. First we will write a "flatten list" function that will help us take a list of lists and make it into a singular list:
\begin{lstlisting}[language=Python, caption=Flatten List Function]
def flatten_list(given_list:list[list[any]]) -> list[any]:
    return [element for inner_list in given_list for element in inner_list]
\end{lstlisting}

This way we can combine all our rows into one single list and perform calculations on it. Since every token has to have a pos and ner tag to it, we can assume that all lists are of the same size at the end. To more easily access our data and to speed up our queries, we will create our own DataRow and DataCollection items, as to more easily manipulate the data.

\begin{lstlisting}[language=Python, caption=Data Row and Data Collection]
class DataRow:
    def __init__(self, tokens, pos, ner, row_idx=0):
        self.idx:int = row_idx
        self.tokens:list[str] = tokens
        self.pos:list[str] = pos
        self.ner:list = ner

class DataCollection:
    def __init__(self, list_collection:list[DataRow], max_token_length=512):
        self.max_token_length:int = max_token_length # max token length (if we tokenize inputs)
        self.collection:list[DataRow] = list_collection # list of rows in the collection

    # get a list of token rows
    def get_token_list(self) -> list[list[str]]:
        return [data_item.tokens for data_item in self.collection]

    # get a list of pos rows
    def get_pos_list(self) -> list[list[str]]:
        return [data_item.pos for data_item in self.collection]

    # get a list of ner rows
    def get_ner_list(self) -> list[list[str]]:
        return [data_item.ner for data_item in self.collection]
\end{lstlisting}

By getting each of the lists, we discover that the train dataset has 40,000 tokens, and validation and test sets both have 5000 tokens each. This leads us to 50000 total samples, from which 40000 we use to train on and another 5000 to test while training.

\includegraphics[scale=0.8]{token_count}

As we will be training on the train dataset, we will want to know how many of the 40000 tokens are actually unique. To do this, we will build a function to make a dictionary of unique tokens, as well as to get the frequency of each token.

\begin{lstlisting}[language=Python, caption=Getting Token Frequency]
def get_word_frequency(data_list:list[str]) -> dict:
    word_frequency:dict = {"total_tokens":0, "unique_tokens":0, "unique_token_frequency":{}}
    for value in data_list:
        if value not in word_frequency["unique_token_frequency"].keys():
            word_frequency["unique_token_frequency"][value] = 1
            word_frequency["unique_tokens"] += 1
        else:
            word_frequency["unique_token_frequency"][value] += 1
        word_frequency["total_tokens"] += 1
    return word_frequency
\end{lstlisting}

By looking at the unique tokens in the train set, we find that out of the 40000 samples, we actually only have 9133 unique tokens. This number is actually smaller as we will discuss in the pre-processing step, but for now these will be our unique tokens. From the frequency we can also get which are the most common tokens.

\begin{lstlisting}[language=Python, caption=Getting Token Frequency]
train_frequency:dict = get_word_frequency(flatten_list(train_collection.get_token_list()))
sorted_list = sorted(train_frequency["unique_token_frequency"].items(), key=lambda t: t[1], reverse=True)
sorted_list = sorted_list[:10]
sorted_dict = {item_tuple[0]:item_tuple[1] for item_tuple in sorted_list}
plt.bar(sorted_dict.keys(), sorted_dict.values(), color=colors)
plt.show() # frequency of classes / labels
\end{lstlisting}

\includegraphics[scale=0.8]{top_10_most_frequent}

From plotting the top 10 most frequent tokens, we can clearly see that the most common tokens are stopwords and punctuations. In order to both balance our classes and focus learning only on abbreviations and long forms, we might benefit from removing them, though we also want to learn what are words that should be labelled as neither, so we will experiment with both.

Next we will want to test how many labels we have available. To do this, we can get use the word frequency function we have already written. This will show us a distribution on the training set of 'B-O': 32971, 'B-LF': 1462, 'I-LF': 3231, 'B-AC': 2336. In a plot it would look as following:

\includegraphics[scale=0.8]{class_distribution}

We can see that we have 4 classes, B-O standing for not an abbreviation or long form, B-AC being an abbreviation, B-LF is long form beginning and I-LF is the continuation of the long form. All long forms will be represented as starting with B-LF followed by I-LF if any. This creates a sequence.

From the plot, we can see that the dataset is imbalanced towards the B-O class. Data imbalance is usually resolved by either oversampling or undersampling the dataset. In the case of oversampling, we would add more values, either from the test or validation set (while removing them from their corresponding set) and adding them to the training set to increase the values of the B-LF, I-LF and B-AC classes. For undersampling we would remove values of B-O. This would be most easily done by removing stop words and their corresponding POS and NER tag values.

\newpage

\section{Experiments}

\subsection{Experiment 1 - Data-Pre processing}

As the first experiment, we will explore pre-processing the data, by using different techniques. To test, we will use the pre-trained DistilBert model from hugging face transformers. Techniques to be tested will be as follows: 

\begin{enumerate}
  \item Clean Stopwords and Punctuations
  \item Stemming
  \item Lemmatization
  \item Removing Special Characters
\end{enumerate}

\subsubsection{Letter Casing}

A common first step in any NLP training is to pre-process the text as the original collection might contain noise. As we have seen from the data analysis, this is true in terms of containing stop words and punctuations.

Let us look at the first 5 words in the first 3 rows:
\begin{center}
\begin{tabular}{||c||} 
    \hline
    First 5 words Per Row \\ [0.5ex] 
    \hline\hline
    ['For', 'this', 'purpose', 'the', 'Gothenburg'] \\ 
    \hline
    ['The', 'following', 'physiological', 'traits', 'were'] \\
    \hline 
    ['Minor', 'H', 'antigen', 'alloimmune', 'responses'] \\
    \hline
\end{tabular}
\end{center}


As our first step, we should make sure that the data is uniform throughout. When we analysed our training tokens, we found that we had 9133 unique tokens. However, this also includes same words with different cased letters. Firstly, let us write a function to lowercase all the tokens:

\begin{lstlisting}[language=Python, caption=Lowercase strings function]
def data_to_lower(data:list[list[str]]) -> list[list[str]]:
    return [[token.lower() for token in tokens] for tokens in data]
\end{lstlisting}

Using the "get word frequency" function we created before, we can now test our original tokens and lower case tokens for unique token count. What we will find is that after lowercasing all of the training dataset tokens, we are left with 8339 unique tokens. 

\subsubsection{Removing punctuations and stopwords}

With our texts being uniform, the next step we can do is to remove all of the punctuations from the text. As they are not words, they increase the B-O count, as well as are one of the most common tokens within the dataset and therefore lean the model to potentially learn more about punctuations than about abbreviations and long forms.

\begin{lstlisting}[language=Python, caption=Code to remove any characters within given list]
def remove_values(data_collection:DataCollection, remove_values:list|str) -> DataCollection:
    collection:list[DataRow] = []
    for data_row in data_collection.collection:
        new_row:DataRow = DataRow([], [], [])
        for item_idx, token in enumerate(data_row.tokens):
            if token not in remove_values:
                new_row.tokens.append(data_row.tokens[item_idx])
                new_row.pos.append(data_row.pos[item_idx])
                new_row.ner.append(data_row.ner[item_idx])
        if len(new_row.tokens) > 0:
            collection.append(new_row)
    new_collection = DataCollection(collection)
    new_collection.set_unique_ner_tags(tag_list)
    return new_collection
\end{lstlisting}

\subsubsection{Removing unique words}

With the above code, we can also create a collection where we remove stopwords and a collection where we remove both stopwords and punctuations. If we remove the stopwords we are left with 31497 tokens. Removing both the stopwords and punctuations leaves us with 22079 words. But what if we were to also only keep unique tokens? We would lose sequential data (as this would not always guarantee B-LF to be followed by I-LF) and we would expect to have worse results in recognition, but to test this out and prove this theory, let us only keep unique words and then also remove stopwords and punctuations.

\begin{lstlisting}[language=Python, caption=Code to make tokens unique only]
def unique_collection(data_collection:DataCollection) -> DataCollection:
    collection:list[DataRow] = []
    unique_tokens:list[str] = []
    for data_row in data_collection.collection:
        new_row:DataRow = DataRow([], [], [])
        for item_idx, token in enumerate(data_row.tokens):
            if token not in unique_tokens:
                new_row.tokens.append(data_row.tokens[item_idx])
                new_row.pos.append(data_row.pos[item_idx])
                new_row.ner.append(data_row.ner[item_idx])
                unique_tokens.append(token)
        if len(new_row.tokens) > 0:
            collection.append(new_row)
    new_collection = DataCollection(collection)
    new_collection.set_unique_ner_tags(tag_list)
    return new_collection
\end{lstlisting}

When running the unique token collection with stop words and punctuations removed, we are left with 8198 tokens. When we plot all of these collections together, we can view their sizes:

\includegraphics[scale=0.8]{data_processing}

\subsubsection{Stemming and Lemmatisation}

Next step is to create 2 more collections for our experiments - stemmed and lemmatisation collections.

Stemming is a text pre-processing technique used to reduce words to their root or base form. The goal of stemming is to simplify and standardize words, which helps improve the performance of text classification. In lemmatisation we also try to reduce a given word to its root word.

The difference between the two is that in stemming we remove last few characters from a word, often leading to incorrect meanings and spelling. However in lemmatisation, we consider the context and convert the word to its meaningful base form, which is called Lemma.

The issue with stemming can be that the word loses its original meaning completely. However, with lemmatising, we might not scale the word back enough to correlate 2 words to be the same. Both have their pros and cons.

In order to do this, we will import the appropriate NLTK libraries

\begin{lstlisting}[language=Python, caption=NLTK libraries for stemming and lemmatising]
from nltk.stem import PorterStemmer
ps = PorterStemmer()

from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
\end{lstlisting}

To stem or lemmatise tokens, we can do the following:

\begin{lstlisting}[language=Python, caption=Tokens to stemmed and lemma]
stemmed_tokens = [[ps.stem(token) for token in token_list]for token_list in train_collection.get_token_list()]
lemmatised_tokens = [[lemmatizer.lemmatize(token) for token in token_list]for token_list in train_collection.get_token_list()]
\end{lstlisting}

The NLTK lemmatiser can also take an additional pos tag value: 
    "n" for nouns,
    "v" for verbs, 
    "a" for adjectives, 
    "r" for adverbs and 
    "s" for satellite adjectives.

With this, we can build a slightly more sophisticated lemmatiser and see if it has a more different effect.

\begin{lstlisting}[language=Python, caption=Lemma with POS handling]
def collection_to_lemma(data_collection:DataCollection) -> DataCollection:
    collection:list[DataRow] = []
    for data_row in data_collection.collection:
        new_row:DataRow = DataRow([], [], [])
        for item_idx, pos in enumerate(data_row.pos):
            token = data_row.tokens[item_idx]
            new_token = ""
            if pos == "NOUN":
                new_token = lemmatizer.lemmatize(token, "n")
            elif pos == "VERB":
                new_token = lemmatizer.lemmatize(token, "v")
            elif pos == "ADJ":
                new_token = lemmatizer.lemmatize(token, "a")
            elif pos == "ADV":
                new_token = lemmatizer.lemmatize(token, "r")
            else:
                new_token = lemmatizer.lemmatize(token)
            new_row.tokens.append(new_token)
            new_row.pos.append(data_row.pos[item_idx])
            new_row.ner.append(data_row.ner[item_idx])
        if len(new_row.tokens) > 0:
            collection.append(new_row)
    new_collection = DataCollection(collection)
    new_collection.set_unique_ner_tags(tag_list)
    return new_collection
\end{lstlisting}

\subsubsection{Removing special characters}

One last thing we can do, is to remove special characters. Sometimes learning these characters can result in the model accuracy dropping and thus we can see if there are any within our dataset and remove them.

\begin{lstlisting}[language=Python, caption=Special character removal]
def is_special_character(token):
    if len(token) > 1:
        return False
    else:
        return ord(token) < 32 or ord(token) > 127 # true if not special character 


special_chars = []
for token in flatten_list(train_collection.get_token_list()):
    if is_special_character(token):
        special_chars.append(token)

train_no_special_collection = remove_values(train_collection, list(set(special_chars)))
\end{lstlisting}

\subsubsection{Evaluation}

Now that we have set up our data for testing, we require a model to test the accuracy on. As such, we will build the Hugging Face Token Classification model for DistilBERT and test our data processed data on that. Since our data is already in collections, it will be easy to add it as we go.

For the model, we will have to build an auto tokeniser, which will both encode and pad our tokens and align labels. It will also give us access to hyper parameterisation. However, we will explain those in more detail in the coming experiments. For now we will focus on only the evaluation of data processing. \\

The model is trained with the following arguments: \\

\begin{tabular}{ |p{5cm}||p{5cm}| }
    \hline
    \multicolumn{2}{|c|}{Training Arguments} \\
    \hline
    Epochs & 50 \\
    \hline
    Learning Rate & 2e-5  \\
    \hline
    Batch Size & 16  \\
    \hline
    Weight Decay & 0.01  \\
    \hline
    Evaluation Strategy & Epoch  \\
    \hline
    Save Strategy & Epoch  \\
    \hline
\end{tabular} \\ \\

As we can see from our results below, training over 50 epochs that our best results come from the orginial text and lemmatisation. We can also see that worst results are from only using unique words while removing stop words and punctuations. This makes sense, as we are removing a lot of data as well as breaking sequential order of B-LF and I-LF, resulting in worse capability of predicting, which lines up with our predictions before.

What is more interesting is that removing stopwords actually does not affect the f1 score too much, but removing punctuations does. This is potentially down to the fact that we have more punctuations than stopwords (as seen in the data analysis section), meaning that we lose more data when removing the punctuations and therefore causing a loss in score. On the other hand, as we do not have many special characters in our training set, removing them barely changes the scoring.

\includegraphics[scale=0.5]{exp1_result}

\begin{tabular}{ |p{6cm}||p{1.5cm}|p{1.5cm}| }
    \hline
    \multicolumn{3}{|c|}{Experiment 1 Results} \\
    \hline
    Test & F1 Score & Time \\
    \hline
    Original & 82\% & 207.11s \\
    \hline
    Lowered & 81\% & 211.40s  \\
    \hline
    Removed Punct & 73\% & 190.46s  \\
    \hline
    Removed Stop & 80\% & 200.66s  \\
    \hline
    Removed Punct/Stop & 72\% & 189.43s  \\
    \hline
    Unique Words, Removed Punct/Stop & 60\% & 193.52s  \\
    \hline
    Stemmed & 80\% & 252.64s  \\
    \hline
    Lemmatised & 82\% & 228.48s  \\
    \hline
    Removed Special & 81\% & 216.06s  \\
    \hline
\end{tabular} \\ \\

\newpage

\subsection{Experiment 2 - Text Encoding / Transformation}

For the second experiment we will test out text vectorisation and see how that will affect the outcome of training. For testing, we will use the pre-trained DistilBert model.

Tokenisers are used to convert raw text into numerical tokens which makes it possible for machine learning models to be understood. When using hugging face models, the AutoTokenizer class makes it easy to select and load appropriate tokenisers for pre-trained models. This simplifies the process of selecting the correct tokeniser associated with our model, streamlining the workflow.

However, to test different tokenisation techniques, we will be also tokenising the text ourselves, using different methods to see how it improves (or unimproves) the training process. The methods we will use for tokenisation are as following:

\begin{enumerate}
  \item Word2vec
  \item Glove
  \item Fasttext
\end{enumerate}















\subsection{Experiment 3 - Hyper Parameters}
DistilBert FineTuning








\subsection{Experiment 4 - Pre-Trained Models}

Spacy vs DistilBert














\subsubsection{Method}
Experimentation with four different experimental setups, where you might be trying out
different options such as (listing more than four different experiments here, so choose four).
Your PDF report should have a subheading for each of the 4 experiments, numbered 2.1, 2.2,
2.3, and 2.4.
\subsubsection{Analysis}
Analyse testing for each of the four experiment variations conducted above (this refers to
accuracy testing, not software testing) – show visuals, such as confusion matrix or other
relevant metrics for each experiment. Use F1-score as primary evaluation metric. Perform an
error analysis on the predictions obtained. Your PDF report should have a subheading for
each of the 4 experiment variations, numbered 3.1, 3.2, 3.3, and 3.4. (20 marks, 5 marks per
variation)












\section{Conclusion}
\subsection{Experiment Result}
Discuss best and worst results from the testing, on all the experiments you conducted and
mention if there was any need to adjust any variables and re-run the experiment (8 marks, 2
mark per variation)











\subsection{Outcome}
Evaluate the overall attempt and outcome – this goes beyond the accuracy of the models, so
some important questions to consider here are:
a. “Can the models you built fulfil their purpose?”
b. “What is good enough F1/accuracy?”
c. If any of the models did not perform well, what is needed to improve?
d. If any of the models performed really well, could/would you make it more efficient
and sacrifice some quality?


\end{document}
