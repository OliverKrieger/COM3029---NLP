{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"surrey-nlp/PLOD-CW\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Parse Datasets from main dataset dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[\"train\"]\n",
    "train_tokens = train_dataset[\"tokens\"]\n",
    "train_pos = train_dataset[\"pos_tags\"]\n",
    "train_ner = train_dataset[\"ner_tags\"]\n",
    "\n",
    "val_dataset = dataset[\"validation\"]\n",
    "val_tokens = val_dataset[\"tokens\"]\n",
    "val_pos = val_dataset[\"pos_tags\"]\n",
    "val_ner = val_dataset[\"ner_tags\"]\n",
    "\n",
    "test_dataset = dataset[\"test\"]\n",
    "test_tokens = test_dataset[\"tokens\"]\n",
    "test_pos = test_dataset[\"pos_tags\"]\n",
    "test_ner = test_dataset[\"ner_tags\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = [\"B-O\", \"B-AC\", \"B-LF\", \"I-LF\"]\n",
    "label_encoding = {\"B-O\": 0, \"B-AC\": 1, \"B-LF\": 2, \"I-LF\": 3}\n",
    "\n",
    "def ner_to_idx(ner_list):\n",
    "    return [[label_encoding[ner] for ner in ner_row] for ner_row in ner_list]\n",
    "\n",
    "train_ner_idx = ner_to_idx(train_ner)\n",
    "val_ner_idx = ner_to_idx(val_ner)\n",
    "test_ner_idx = ner_to_idx(test_ner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olive\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(dataset_tokens, dataset_ner):\n",
    "    tokenized_inputs = tokenizer(dataset_tokens, truncation=True, is_split_into_words=True) ## For some models, you may need to set max_length to approximately 500.\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(dataset_ner):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(label[word_idx])\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_to_list(encoded):\n",
    "    new_list = []\n",
    "\n",
    "    for labels, inputs in zip(encoded[\"labels\"], encoded[\"input_ids\"]):\n",
    "        entry = {\"input_ids\": inputs, \"labels\": labels}\n",
    "        new_list.append(entry)\n",
    "\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olive\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "C:\\Users\\olive\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olive\\AppData\\Local\\Temp\\ipykernel_61332\\3211323836.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"seqeval\")\n",
      "C:\\Users\\olive\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\datasets\\load.py:756: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "metric = load_metric(\"seqeval\")\n",
    "\n",
    "def get_labels(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    return true_predictions, true_labels\n",
    "\n",
    "def get_metrics(true_predictions, true_labels):\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "def compute_metrics(p):\n",
    "    true_predictions, true_labels = get_labels(p)\n",
    "    return get_metrics(true_predictions, true_labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_args(\n",
    "    output_dir=\"bert/bert-base-uncased\",\n",
    "    evaluation_strategy ='epoch',\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit = 1,\n",
    "    learning_rate=2e-5,\n",
    "    batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.001,\n",
    "    metric_for_best_model = 'f1',\n",
    "    load_best_model_at_end=True\n",
    ") -> TrainingArguments:\n",
    "    return TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=num_train_epochs, # number of epochs to train\n",
    "        weight_decay=weight_decay, # The weight decay to apply (if not zero) to all layers except all bias and LayerNorm weights\n",
    "        evaluation_strategy=evaluation_strategy,\n",
    "        save_strategy=save_strategy,\n",
    "        save_total_limit=save_total_limit,\n",
    "        metric_for_best_model=metric_for_best_model,\n",
    "        load_best_model_at_end=load_best_model_at_end,\n",
    "        report_to=[\"none\"]\n",
    "    )\n",
    "\n",
    "def get_trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_data,\n",
    "    eval_data,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    comp_metrics=compute_metrics\n",
    ") -> Trainer:\n",
    "    return Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=eval_data,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=comp_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels to Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def labels_to_df(predicted_labels, true_labels):\n",
    "    df = pd.DataFrame(0, columns=label_list, index=label_list) # create dataframe with only zeroes but all labels!\n",
    "\n",
    "    for true_label, predict_label in zip(true_labels, predicted_labels):\n",
    "        for t, p in zip(true_label, predict_label):\n",
    "            df.at[t, p] += 1 # count amount of labels\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_train_and_predict(\n",
    "        train_tokens,\n",
    "        train_ner,\n",
    "        val_tokens,\n",
    "        val_ner,\n",
    "        test_tokens,\n",
    "        test_ner,\n",
    "        training_args=None,\n",
    "    ) -> pd.DataFrame:\n",
    "    tokenised_train = encoding_to_list(tokenize_and_align_labels(train_tokens, train_ner))\n",
    "    tokenised_val = encoding_to_list(tokenize_and_align_labels(val_tokens, val_ner))\n",
    "    tokenised_test = encoding_to_list(tokenize_and_align_labels(test_tokens, test_ner))\n",
    "\n",
    "    training_args = get_training_args() if training_args == None else training_args\n",
    "    \n",
    "    model = AutoModelForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=4)\n",
    "\n",
    "    trainer = get_trainer(\n",
    "        model=model,\n",
    "        training_args=training_args,\n",
    "        train_data=tokenised_train,\n",
    "        eval_data=tokenised_val)\n",
    "    trainer.train()\n",
    "\n",
    "    p, l, _ = trainer.predict(tokenised_test)\n",
    "    # predicted_labels, true_labels = get_labels((p,l))\n",
    "    return p, l\n",
    "\n",
    "\n",
    "def predict_to_dataframe(p, l):\n",
    "    predicted_labels, true_labels = get_labels((p,l))\n",
    "    metrics = get_metrics(predicted_labels, true_labels)\n",
    "\n",
    "    df = labels_to_df(predicted_labels, true_labels)\n",
    "\n",
    "    print(\"Metrics:\\n\", metrics)\n",
    "    print(\"Dataframe:\\n\", df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\olive\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\accelerate\\accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df9bbb6eb4a04d68967aba4d934986a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/67 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "785642924b46455ab9362e3add58233b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory bert/bert-base-uncased\\checkpoint-67 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.30527928471565247, 'eval_precision': 0.898586456278764, 'eval_recall': 0.9016988289625598, 'eval_f1': 0.9001399522515847, 'eval_accuracy': 0.8857840891739197, 'eval_runtime': 0.3435, 'eval_samples_per_second': 366.776, 'eval_steps_per_second': 23.287, 'epoch': 1.0}\n",
      "{'train_runtime': 8.4913, 'train_samples_per_second': 126.247, 'train_steps_per_second': 7.89, 'train_loss': 0.5440893030878323, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad3c60b1c9294172a14e5368e5816924",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics:\n",
      " {'precision': 0.9004329004329005, 'recall': 0.899135446685879, 'f1': 0.8997837058399424, 'accuracy': 0.8848293489700797}\n",
      "Dataframe:\n",
      "        B-O  B-AC  B-LF  I-LF\n",
      "B-O   5130   175     6    86\n",
      "B-AC   134   410     0     3\n",
      "B-LF    83     6    35   176\n",
      "I-LF    89     7     1   310\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B-O</th>\n",
       "      <th>B-AC</th>\n",
       "      <th>B-LF</th>\n",
       "      <th>I-LF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>B-O</th>\n",
       "      <td>5130</td>\n",
       "      <td>175</td>\n",
       "      <td>6</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-AC</th>\n",
       "      <td>134</td>\n",
       "      <td>410</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-LF</th>\n",
       "      <td>83</td>\n",
       "      <td>6</td>\n",
       "      <td>35</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-LF</th>\n",
       "      <td>89</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       B-O  B-AC  B-LF  I-LF\n",
       "B-O   5130   175     6    86\n",
       "B-AC   134   410     0     3\n",
       "B-LF    83     6    35   176\n",
       "I-LF    89     7     1   310"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p, l = run_train_and_predict(\n",
    "    train_tokens,\n",
    "    train_ner_idx,\n",
    "    val_tokens,\n",
    "    val_ner_idx,\n",
    "    test_tokens,\n",
    "    test_ner_idx\n",
    ")\n",
    "\n",
    "DataFrame = predict_to_dataframe(p, l)\n",
    "DataFrame\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
