{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict, Dataset # Import dataset import function for hugging face\n",
    "dataset:DatasetDict = load_dataset(\"surrey-nlp/PLOD-CW\") # import the coursework dataset from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training usage https://spacy.io/usage/training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = dataset[\"train\"]\n",
    "train_tokens = train_dict[\"tokens\"]\n",
    "train_pos_tags = train_dict[\"pos_tags\"]\n",
    "train_ner_tags = train_dict[\"ner_tags\"]\n",
    "\n",
    "validation_dict = dataset[\"validation\"]\n",
    "validation_tokens = validation_dict[\"tokens\"]\n",
    "validation_pos_tags = validation_dict[\"pos_tags\"]\n",
    "validation_ner_tags = validation_dict[\"ner_tags\"]\n",
    "\n",
    "test_dict = dataset[\"test\"]\n",
    "test_tokens = test_dict[\"tokens\"]\n",
    "test_pos_tags = test_dict[\"pos_tags\"]\n",
    "test_ner_tags = test_dict[\"ner_tags\"]\n",
    "\n",
    "def flatten_list(given_list:list[list[any]]) -> list[any]:\n",
    "    return [element for inner_list in given_list for element in inner_list]\n",
    "\n",
    "def data_to_lower(data:list[list[str]]) -> list[list[str]]:\n",
    "    return [[token.lower() for token in tokens] for tokens in data]\n",
    "\n",
    "train_tokens = data_to_lower(train_tokens)\n",
    "validation_tokens = data_to_lower(validation_tokens)\n",
    "test_tokens = data_to_lower(test_tokens)\n",
    "\n",
    "class DataItem:\n",
    "    def __init__(self, tokens, pos, ner, idx=0):\n",
    "        self.idx=idx\n",
    "        self.tokens:list[str] = tokens\n",
    "        self.pos:list[str] = pos\n",
    "        self.ner:list = ner\n",
    "\n",
    "class DataCollection:\n",
    "    def __init__(self, data_collection:list[DataItem], max_token_length=512):\n",
    "        self.max_token_length = max_token_length\n",
    "        self.data_collection:list[DataItem] = data_collection\n",
    "        self.unique_tags = self.get_unique_tags()\n",
    "        self.item_embeddings:dict = self.create_item_embeddings(self.unique_tags)\n",
    "        self.reverse_embeddings:dict = {v:k for k,v in self.item_embeddings.items()}\n",
    "\n",
    "    def get_token_list(self) -> list[list[str]]:\n",
    "        return [data_item.tokens for data_item in self.data_collection]\n",
    "\n",
    "    def get_pos_list(self) -> list[list[str]]:\n",
    "        return [data_item.pos for data_item in self.data_collection]\n",
    "\n",
    "    def get_ner_list(self) -> list[list[str]]:\n",
    "        return [data_item.ner for data_item in self.data_collection]\n",
    "    \n",
    "    def get_ner_idx_list(self) -> list[list[str]]:\n",
    "        ner_idx_list_collection = []\n",
    "        for data_item in self.data_collection:\n",
    "            ner_idx_list = []\n",
    "            for ner_tag in data_item.ner:\n",
    "                ner_idx_list.append(self.item_embeddings[ner_tag])\n",
    "            ner_idx_list_collection.append(ner_idx_list)\n",
    "        return ner_idx_list_collection\n",
    "\n",
    "    \n",
    "    def get_unique_tags(self) -> list[str]:\n",
    "        return flatten_list(self.get_ner_list())\n",
    "    \n",
    "    def create_item_embeddings(self, tags:list[str]) -> dict:\n",
    "        return {label:idx for idx, label in enumerate(tags)}\n",
    "\n",
    "def dataset_to_collection(dataset:Dataset) -> DataCollection:\n",
    "    data_items:list[DataItem] = []\n",
    "    for idx in range(dataset.num_rows):\n",
    "        data_items.append(DataItem(dataset[\"tokens\"][idx], dataset[\"pos_tags\"][idx], dataset[\"ner_tags\"][idx], idx))\n",
    "    return DataCollection(data_items)\n",
    "\n",
    "train_data:list[DataItem] = []\n",
    "for idx in range(len(train_tokens)):\n",
    "    train_data.append(DataItem(train_tokens[idx], train_pos_tags[idx], train_ner_tags[idx], idx))\n",
    "train_collection:DataCollection = DataCollection(train_data)\n",
    "\n",
    "validation_data:list[DataItem] = []\n",
    "for idx in range(len(validation_tokens)):\n",
    "    validation_data.append(DataItem(validation_tokens[idx], validation_pos_tags[idx], validation_ner_tags[idx], idx))\n",
    "validation_collection:DataCollection = DataCollection(validation_data)\n",
    "\n",
    "test_data:list[DataItem] = []\n",
    "for idx in range(len(test_tokens)):\n",
    "    test_data.append(DataItem(test_tokens[idx], test_pos_tags[idx], test_ner_tags[idx], idx))\n",
    "test_collection:DataCollection = DataCollection(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string  \n",
    "\n",
    "# spacy\n",
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Doc, DocBin\n",
    "nlp:Language = spacy.blank(\"en\")\n",
    "\n",
    "# configuration\n",
    "def create_dir(dir_path):\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "\n",
    "working_dir:str = os.getcwd()\n",
    "vocab_dir:str = os.path.join(working_dir, \"spacy_vocab\")\n",
    "create_dir(vocab_dir)\n",
    "\n",
    "config_dir:str = os.path.join(working_dir, \"config\")\n",
    "create_dir(config_dir)\n",
    "\n",
    "output_dir:str = os.path.join(working_dir, \"output\")\n",
    "create_dir(output_dir)\n",
    "\n",
    "train_vocab_path = os.path.join(vocab_dir, \"train.spacy\")\n",
    "dev_vocab_path = os.path.join(vocab_dir, \"dev.spacy\")\n",
    "config_path = os.path.join(config_dir, \"config.cfg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docbin:DocBin = DocBin()\n",
    "test_docbin:DocBin = DocBin()\n",
    "\n",
    "def dataset_to_vocab(collection:DataCollection, doc_bin:DocBin) -> dict:\n",
    "    # vocab = {\"words\":[], \"spaces\":[], \"ents\":[]}\n",
    "\n",
    "    for data_item in collection.data_collection:\n",
    "        spaces = [True if token not in string.punctuation else False for token in data_item.tokens] \n",
    "        doc = Doc(nlp.vocab, words=data_item.tokens, spaces=spaces, ents=data_item.ner)\n",
    "        doc_bin.add(doc)\n",
    "\n",
    "dataset_to_vocab(train_collection, train_docbin)\n",
    "dataset_to_vocab(test_collection, test_docbin)\n",
    "\n",
    "train_docbin.to_disk(train_vocab_path)\n",
    "test_docbin.to_disk(dev_vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Saving to output directory:\n",
      "c:\\Users\\olive\\OneDrive\\Documents\\Uni\\COM3029 - NLP\\cw\\spacy\\output\u001b[0m\n",
      "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['transformer', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
      "E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  -------------  --------  ------  ------  ------  ------\n",
      "  0       0        5584.67    866.56    0.50    2.46    0.28    0.00\n",
      "  7     200      134885.64  67008.82   92.29   93.96   90.69    0.92\n",
      " 14     400        6795.98  10354.62   92.54   93.66   91.45    0.93\n",
      " 22     600        1644.44   2583.32   92.21   93.85   90.62    0.92\n",
      " 30     800         761.92   1126.84   91.78   92.99   90.60    0.92\n",
      " 37    1000         545.64    794.82   92.29   93.88   90.75    0.92\n",
      " 45    1200         409.84    585.76   92.14   93.72   90.60    0.92\n",
      " 52    1400         271.97    379.20   92.64   93.69   91.62    0.93\n",
      " 60    1600         307.55    456.88   92.57   93.83   91.34    0.93\n",
      " 67    1800         373.42    485.84   92.57   94.16   91.03    0.93\n",
      " 75    2000         202.04    267.02   92.53   93.83   91.26    0.93\n",
      " 82    2200         211.21    255.95   92.58   93.97   91.24    0.93\n",
      " 90    2400         131.07    180.97   92.67   94.15   91.24    0.93\n",
      " 97    2600         134.28    155.39   92.44   93.70   91.22    0.92\n",
      "105    2800          76.83     98.49   92.80   94.03   91.60    0.93\n",
      "112    3000         195.38    181.13   92.29   93.82   90.81    0.92\n",
      "119    3200         146.70    132.90   92.58   94.01   91.20    0.93\n",
      "127    3400         110.82    122.28   92.37   93.98   90.81    0.92\n",
      "134    3600         189.13    159.21   92.41   93.85   91.01    0.92\n",
      "142    3800         131.14    137.88   92.73   93.85   91.64    0.93\n",
      "149    4000          87.21     85.87   92.71   94.04   91.41    0.93\n",
      "156    4200          76.48     68.29   92.52   93.89   91.20    0.93\n",
      "164    4400         112.49    110.02   92.68   94.18   91.24    0.93\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "c:\\Users\\olive\\OneDrive\\Documents\\Uni\\COM3029 - NLP\\cw\\spacy\\output\\model-last\n"
     ]
    }
   ],
   "source": [
    "from spacy.cli.train import train\n",
    "\n",
    "train(config_path=config_path, output_path=output_dir, overrides={\"paths.train\": train_vocab_path, \"paths.dev\": dev_vocab_path}, use_gpu=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
