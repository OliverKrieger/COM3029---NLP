{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict, Dataset # Import dataset import function for hugging face\n",
    "dataset:DatasetDict = load_dataset(\"surrey-nlp/PLOD-CW\") # import the coursework dataset from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training usage https://spacy.io/usage/training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = dataset[\"train\"]\n",
    "train_tokens = train_dict[\"tokens\"]\n",
    "train_pos_tags = train_dict[\"pos_tags\"]\n",
    "train_ner_tags = train_dict[\"ner_tags\"]\n",
    "\n",
    "validation_dict = dataset[\"validation\"]\n",
    "validation_tokens = validation_dict[\"tokens\"]\n",
    "validation_pos_tags = validation_dict[\"pos_tags\"]\n",
    "validation_ner_tags = validation_dict[\"ner_tags\"]\n",
    "\n",
    "test_dict = dataset[\"test\"]\n",
    "test_tokens = test_dict[\"tokens\"]\n",
    "test_pos_tags = test_dict[\"pos_tags\"]\n",
    "test_ner_tags = test_dict[\"ner_tags\"]\n",
    "\n",
    "def flatten_list(given_list:list[list[any]]) -> list[any]:\n",
    "    return [element for inner_list in given_list for element in inner_list]\n",
    "\n",
    "def data_to_lower(data:list[list[str]]) -> list[list[str]]:\n",
    "    return [[token.lower() for token in tokens] for tokens in data]\n",
    "\n",
    "train_tokens = data_to_lower(train_tokens)\n",
    "validation_tokens = data_to_lower(validation_tokens)\n",
    "test_tokens = data_to_lower(test_tokens)\n",
    "\n",
    "class DataItem:\n",
    "    def __init__(self, tokens, pos, ner, idx=0):\n",
    "        self.idx=idx\n",
    "        self.tokens:list[str] = tokens\n",
    "        self.pos:list[str] = pos\n",
    "        self.ner:list = ner\n",
    "\n",
    "class DataCollection:\n",
    "    def __init__(self, data_collection:list[DataItem], max_token_length=512):\n",
    "        self.max_token_length = max_token_length\n",
    "        self.data_collection:list[DataItem] = data_collection\n",
    "        self.unique_tags = self.get_unique_tags()\n",
    "        self.item_embeddings:dict = self.create_item_embeddings(self.unique_tags)\n",
    "        self.reverse_embeddings:dict = {v:k for k,v in self.item_embeddings.items()}\n",
    "\n",
    "    def get_token_list(self) -> list[list[str]]:\n",
    "        return [data_item.tokens for data_item in self.data_collection]\n",
    "\n",
    "    def get_pos_list(self) -> list[list[str]]:\n",
    "        return [data_item.pos for data_item in self.data_collection]\n",
    "\n",
    "    def get_ner_list(self) -> list[list[str]]:\n",
    "        return [data_item.ner for data_item in self.data_collection]\n",
    "    \n",
    "    def get_ner_idx_list(self) -> list[list[str]]:\n",
    "        ner_idx_list_collection = []\n",
    "        for data_item in self.data_collection:\n",
    "            ner_idx_list = []\n",
    "            for ner_tag in data_item.ner:\n",
    "                ner_idx_list.append(self.item_embeddings[ner_tag])\n",
    "            ner_idx_list_collection.append(ner_idx_list)\n",
    "        return ner_idx_list_collection\n",
    "\n",
    "    \n",
    "    def get_unique_tags(self) -> list[str]:\n",
    "        return flatten_list(self.get_ner_list())\n",
    "    \n",
    "    def create_item_embeddings(self, tags:list[str]) -> dict:\n",
    "        return {label:idx for idx, label in enumerate(tags)}\n",
    "\n",
    "def dataset_to_collection(dataset:Dataset) -> DataCollection:\n",
    "    data_items:list[DataItem] = []\n",
    "    for idx in range(dataset.num_rows):\n",
    "        data_items.append(DataItem(dataset[\"tokens\"][idx], dataset[\"pos_tags\"][idx], dataset[\"ner_tags\"][idx], idx))\n",
    "    return DataCollection(data_items)\n",
    "\n",
    "train_data:list[DataItem] = []\n",
    "for idx in range(len(train_tokens)):\n",
    "    train_data.append(DataItem(train_tokens[idx], train_pos_tags[idx], train_ner_tags[idx], idx))\n",
    "train_collection:DataCollection = DataCollection(train_data)\n",
    "\n",
    "validation_data:list[DataItem] = []\n",
    "for idx in range(len(validation_tokens)):\n",
    "    validation_data.append(DataItem(validation_tokens[idx], validation_pos_tags[idx], validation_ner_tags[idx], idx))\n",
    "validation_collection:DataCollection = DataCollection(validation_data)\n",
    "\n",
    "test_data:list[DataItem] = []\n",
    "for idx in range(len(test_tokens)):\n",
    "    test_data.append(DataItem(test_tokens[idx], test_pos_tags[idx], test_ner_tags[idx], idx))\n",
    "test_collection:DataCollection = DataCollection(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olive\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "C:\\Users\\olive\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "# python\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string  \n",
    "\n",
    "# spacy\n",
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Doc, DocBin\n",
    "nlp:Language = spacy.blank(\"en\")\n",
    "\n",
    "# configuration\n",
    "def create_dir(dir_path):\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "\n",
    "working_dir:str = os.getcwd()\n",
    "vocab_dir:str = os.path.join(working_dir, \"spacy_vocab\")\n",
    "create_dir(vocab_dir)\n",
    "\n",
    "config_dir:str = os.path.join(working_dir, \"config\")\n",
    "create_dir(config_dir)\n",
    "\n",
    "output_dir:str = os.path.join(working_dir, \"output\")\n",
    "create_dir(output_dir)\n",
    "\n",
    "train_vocab_path = os.path.join(vocab_dir, \"train.spacy\")\n",
    "dev_vocab_path = os.path.join(vocab_dir, \"dev.spacy\")\n",
    "config_path = os.path.join(config_dir, \"config.cfg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docbin:DocBin = DocBin()\n",
    "test_docbin:DocBin = DocBin()\n",
    "\n",
    "def dataset_to_vocab(collection:DataCollection, doc_bin:DocBin) -> dict:\n",
    "    for data_item in collection.data_collection:\n",
    "        spaces = [True if token not in string.punctuation else False for token in data_item.tokens] \n",
    "        doc = Doc(nlp.vocab, words=data_item.tokens, spaces=spaces, ents=data_item.ner)\n",
    "        doc_bin.add(doc)\n",
    "\n",
    "dataset_to_vocab(train_collection, train_docbin)\n",
    "dataset_to_vocab(test_collection, test_docbin)\n",
    "\n",
    "train_docbin.to_disk(train_vocab_path)\n",
    "test_docbin.to_disk(dev_vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Saving to output directory:\n",
      "c:\\Users\\olive\\OneDrive\\Documents\\Uni\\COM3029 - NLP\\cw\\spacy\\output\u001b[0m\n",
      "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['transformer', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
      "E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  -------------  --------  ------  ------  ------  ------\n",
      "  0       0        5584.67    866.56    0.50    2.46    0.28    0.00\n",
      "  7     200      134885.64  67008.82   92.29   93.96   90.69    0.92\n",
      " 14     400        6795.98  10354.62   92.54   93.66   91.45    0.93\n",
      " 22     600        1644.44   2583.32   92.21   93.85   90.62    0.92\n",
      " 30     800         761.92   1126.84   91.78   92.99   90.60    0.92\n",
      " 37    1000         545.64    794.82   92.29   93.88   90.75    0.92\n",
      " 45    1200         409.84    585.76   92.14   93.72   90.60    0.92\n",
      " 52    1400         271.97    379.20   92.64   93.69   91.62    0.93\n",
      " 60    1600         307.55    456.88   92.57   93.83   91.34    0.93\n",
      " 67    1800         373.42    485.84   92.57   94.16   91.03    0.93\n",
      " 75    2000         202.04    267.02   92.53   93.83   91.26    0.93\n",
      " 82    2200         211.21    255.95   92.58   93.97   91.24    0.93\n",
      " 90    2400         131.07    180.97   92.67   94.15   91.24    0.93\n",
      " 97    2600         134.28    155.39   92.44   93.70   91.22    0.92\n",
      "105    2800          76.83     98.49   92.80   94.03   91.60    0.93\n",
      "112    3000         195.38    181.13   92.29   93.82   90.81    0.92\n",
      "119    3200         146.70    132.90   92.58   94.01   91.20    0.93\n",
      "127    3400         110.82    122.28   92.37   93.98   90.81    0.92\n",
      "134    3600         189.13    159.21   92.41   93.85   91.01    0.92\n",
      "142    3800         131.14    137.88   92.73   93.85   91.64    0.93\n",
      "149    4000          87.21     85.87   92.71   94.04   91.41    0.93\n",
      "156    4200          76.48     68.29   92.52   93.89   91.20    0.93\n",
      "164    4400         112.49    110.02   92.68   94.18   91.24    0.93\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "c:\\Users\\olive\\OneDrive\\Documents\\Uni\\COM3029 - NLP\\cw\\spacy\\output\\model-last\n"
     ]
    }
   ],
   "source": [
    "# from spacy.cli.train import train\n",
    "\n",
    "# train(config_path=config_path, output_path=output_dir, overrides={\"paths.train\": train_vocab_path, \"paths.dev\": dev_vocab_path}, use_gpu=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olive\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "best_model_nlp:Language = spacy.load(\"./output/model-best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25  IDX was padded to match! Span Len:  40  | Tok Len:  40\n",
      "32  IDX was padded to match! Span Len:  50  | Tok Len:  50\n",
      "47  IDX was padded to match! Span Len:  67  | Tok Len:  67\n",
      "First Invalid\n",
      "[”[6] | [”] | [”]\n",
      "Mid Invalid\n",
      "[”[6] | [[] | [”[6]\n",
      "Final invalid\n",
      "[”[6] | [6] | [”[6]\n",
      "Valid\n",
      "[]] | []] | [6]\n",
      "Valid\n",
      "[this] | [this] | [this]\n",
      "Valid\n",
      "[value] | [value] | [value]\n",
      "Valid\n",
      "[is] | [is] | [is]\n",
      "Valid\n",
      "[in] | [in] | [in]\n",
      "Valid\n",
      "[excellent] | [excellent] | [excellent]\n",
      "Valid\n",
      "[agreement] | [agreement] | [agreement]\n",
      "Valid\n",
      "[with] | [with] | [with]\n",
      "Valid\n",
      "[the] | [the] | [the]\n",
      "Valid\n",
      "[expected] | [expected] | [expected]\n",
      "Valid\n",
      "[molecular] | [molecular] | [molecular]\n",
      "Valid\n",
      "[weight] | [weight] | [weight]\n",
      "Valid\n",
      "[of] | [of] | [of]\n",
      "Valid\n",
      "[the] | [the] | [the]\n",
      "Valid\n",
      "[holoenzyme] | [holoenzyme] | [holoenzyme]\n",
      "Valid\n",
      "[,] | [,] | [,]\n",
      "Valid\n",
      "[28,705.88] | [28,705.88] | [28,705.88]\n",
      "Valid\n",
      "[da] | [da] | [da]\n",
      "Valid\n",
      "[.] | [.] | [.]\n",
      "TO: ['”[6', ']', 'this', 'value', 'is', 'in', 'excellent', 'agreement', 'with', 'the', 'expected', 'molecular', 'weight', 'of', 'the', 'holoenzyme', ',', '28,705.88', 'da', '.']\n",
      "TL: ['”[6', 'inv', 'inv', ']', 'this', 'value', 'is', 'in', 'excellent', 'agreement', 'with', 'the', 'expected', 'molecular', 'weight', 'of', 'the', 'holoenzyme', ',', '28,705.88', 'da', '.']\n",
      "PR: ['”', '[', '6', ']', 'this', 'value', 'is', 'in', 'excellent', 'agreement', 'with', 'the', 'expected', 'molecular', 'weight', 'of', 'the', 'holoenzyme', ',', '28,705.88', 'da', '.']\n",
      "68  Tokens Padded Successfully!\n",
      "68  IDX was padded to match! Span Len:  22  | Tok Len:  22\n",
      "83  IDX was padded to match! Span Len:  35  | Tok Len:  35\n",
      "96  IDX was padded to match! Span Len:  35  | Tok Len:  35\n",
      "110  IDX was padded to match! Span Len:  8  | Tok Len:  8\n",
      "Valid\n",
      "[additionally] | [additionally] | [additionally]\n",
      "Valid\n",
      "[,] | [,] | [additionally]\n",
      "Valid\n",
      "[to] | [to] | [to]\n",
      "Valid\n",
      "[investigate] | [investigate] | [investigate]\n",
      "Valid\n",
      "[the] | [the] | [the]\n",
      "Valid\n",
      "[differences] | [differences] | [differences]\n",
      "Valid\n",
      "[between] | [between] | [between]\n",
      "Valid\n",
      "[sleep] | [sleep] | [sleep]\n",
      "Valid\n",
      "[and] | [and] | [and]\n",
      "Valid\n",
      "[n] | [n] | [n]\n",
      "Valid\n",
      "[animals] | [animals] | [animals]\n",
      "Valid\n",
      "[at] | [at] | [at]\n",
      "Valid\n",
      "[retrieval] | [retrieval] | [retrieval]\n",
      "Valid\n",
      "[,] | [,] | [,]\n",
      "Valid\n",
      "[we] | [we] | [we]\n",
      "Valid\n",
      "[calculated] | [calculated] | [calculated]\n",
      "Valid\n",
      "[percentage] | [percentage] | [percentage]\n",
      "Valid\n",
      "[change] | [change] | [change]\n",
      "Valid\n",
      "[(] | [(] | [(]\n",
      "Valid\n",
      "[sleep] | [sleep] | [sleep]\n",
      "Valid\n",
      "[>] | [>] | [>]\n",
      "First Invalid\n",
      "[+sdn] | [+] | [+]\n",
      "Final invalid\n",
      "[+sdn] | [sdn] | [+sdn]\n",
      "Valid\n",
      "[)] | [)] | [sdn]\n",
      "Valid\n",
      "[on] | [on] | [on]\n",
      "Valid\n",
      "[each] | [each] | [each]\n",
      "Valid\n",
      "[plate] | [plate] | [plate]\n",
      "Valid\n",
      "[and] | [and] | [and]\n",
      "Valid\n",
      "[then] | [then] | [then]\n",
      "Valid\n",
      "[averaged] | [averaged] | [averaged]\n",
      "Valid\n",
      "[the] | [the] | [the]\n",
      "Valid\n",
      "[data] | [data] | [data]\n",
      "Valid\n",
      "[(] | [(] | [(]\n",
      "Valid\n",
      "[see] | [see] | [see]\n",
      "Valid\n",
      "[s7] | [s7] | [s7]\n",
      "Valid\n",
      "[fig] | [fig] | [fig]\n",
      "Valid\n",
      "[)] | [)] | [)]\n",
      "Valid\n",
      "[.] | [.] | [.]\n",
      "TO: ['additionally', ',', 'to', 'investigate', 'the', 'differences', 'between', 'sleep', 'and', 'n', 'animals', 'at', 'retrieval', ',', 'we', 'calculated', 'percentage', 'change', '(', 'sleep', '>', '+sdn', ')', 'on', 'each', 'plate', 'and', 'then', 'averaged', 'the', 'data', '(', 'see', 's7', 'fig', ')', '.']\n",
      "TL: ['additionally', ',', 'to', 'investigate', 'the', 'differences', 'between', 'sleep', 'and', 'n', 'animals', 'at', 'retrieval', ',', 'we', 'calculated', 'percentage', 'change', '(', 'sleep', '>', '+sdn', 'inv', ')', 'on', 'each', 'plate', 'and', 'then', 'averaged', 'the', 'data', '(', 'see', 's7', 'fig', ')', '.']\n",
      "PR: ['additionally', ',', 'to', 'investigate', 'the', 'differences', 'between', 'sleep', 'and', 'n', 'animals', 'at', 'retrieval', ',', 'we', 'calculated', 'percentage', 'change', '(', 'sleep', '>', '+', 'sdn', ')', 'on', 'each', 'plate', 'and', 'then', 'averaged', 'the', 'data', '(', 'see', 's7', 'fig', ')', '.']\n",
      "124  Tokens Padded Successfully!\n",
      "124  IDX was padded to match! Span Len:  38  | Tok Len:  38\n"
     ]
    }
   ],
   "source": [
    "class EntSpan:\n",
    "    def __init__(self, text, label):\n",
    "        self.text = text\n",
    "        self.label = label\n",
    "\n",
    "def ents_to_len(ents) -> int:\n",
    "    tok_len = 0\n",
    "    for ent in ents:\n",
    "        if len(ent.text) > 1:\n",
    "            ent_tokens = ent.text.split(\" \")\n",
    "            tok_len += len(ent_tokens)\n",
    "        else:\n",
    "            tok_len += 1\n",
    "    return tok_len\n",
    "\n",
    "def ent_to_entspan(ents) -> list[EntSpan]:\n",
    "    span_list:list[EntSpan] = []\n",
    "    for ent in ents:\n",
    "        if len(ent.text) > 1: # handle multiple tokens split\n",
    "            ent_tokens = ent.text.split(\" \")\n",
    "            if ent.label_ == \"LF\": # handle LF split\n",
    "                for idx, tok in enumerate(ent_tokens):\n",
    "                    if idx == 0:\n",
    "                        span_list.append(EntSpan(tok, (\"B-\"+str(ent.label_))))\n",
    "                    else:\n",
    "                        span_list.append(EntSpan(tok, (\"I-\"+str(ent.label_))))\n",
    "            else:\n",
    "                for tok in ent_tokens:\n",
    "                    span_list.append(EntSpan(tok, (\"B-\"+str(ent.label_))))\n",
    "        else:\n",
    "            span_list.append(EntSpan(ent.text, (\"B-\"+str(ent.label_))))\n",
    "\n",
    "    return span_list\n",
    "\n",
    "def pad_ents(tokens:list[str], ents) -> list[EntSpan]:\n",
    "    span_list:list[EntSpan] = ent_to_entspan(ents)\n",
    "    invalid_token = \"invalid\" # for ease of use to change what is invalid token naming convention\n",
    "\n",
    "    for idx in range(len(tokens)):\n",
    "        if tokens[idx] != span_list[idx].text:\n",
    "            span_list.insert(idx, EntSpan(tokens[idx], invalid_token))\n",
    "\n",
    "    return span_list\n",
    "\n",
    "def pad_tokens(tokens:list[str], ents) -> list[str]: # need to pad tokens, as\n",
    "    span_list:list[EntSpan] = ent_to_entspan(ents)\n",
    "    token_list:list[str] = []\n",
    "    invalid_token = \"inv\"\n",
    "\n",
    "    next_token:str = str(span_list[0].text) # set it to first token!\n",
    "    token_idx = 0\n",
    "    invalid_tok = False\n",
    "    for idx in range(len(span_list)):\n",
    "        if tokens[token_idx] != span_list[idx].text:\n",
    "            if tokens[token_idx] != next_token and invalid_tok: # second part is to avoid overriding the first value!\n",
    "                print(\"Mid Invalid\")\n",
    "                next_token += str(span_list[idx+1].text) # append prev\n",
    "                print(f\"[{tokens[token_idx]}] | [{span_list[idx].text}] | [{next_token}]\")\n",
    "                token_list.insert(idx, invalid_token)\n",
    "            elif tokens[token_idx] == next_token:\n",
    "                print(\"Final invalid\")\n",
    "                print(f\"[{tokens[token_idx]}] | [{span_list[idx].text}] | [{next_token}]\")\n",
    "                token_list.insert(idx, invalid_token)\n",
    "                token_idx += 1\n",
    "                next_token = str(span_list[idx].text) # clear prev\n",
    "                invalid_tok = False\n",
    "            else:\n",
    "                print(\"First Invalid\")\n",
    "                print(f\"[{tokens[token_idx]}] | [{span_list[idx].text}] | [{next_token}]\")\n",
    "                token_list.insert(idx, tokens[token_idx])\n",
    "                next_token += str(span_list[idx+1].text) # clear prev\n",
    "                invalid_tok = True\n",
    "        else:\n",
    "            print(\"Valid\")\n",
    "            print(f\"[{tokens[token_idx]}] | [{span_list[idx].text}] | [{next_token}]\")\n",
    "            if idx == 0:\n",
    "                next_token = span_list[idx].text\n",
    "            elif idx < len(span_list)-1:\n",
    "                next_token = span_list[idx+1].text\n",
    "            else:\n",
    "                next_token = \"\"\n",
    "            token_list.insert(idx, tokens[token_idx])\n",
    "            token_idx += 1\n",
    "    print(\"TO:\", tokens)\n",
    "    print(\"TL:\", token_list)\n",
    "    print(\"PR:\", [str(ent.text) for ent in ents])\n",
    "    return token_list\n",
    "\n",
    "def collection_to_predictions(data_collection:DataCollection):\n",
    "    validation_token_list = data_collection.get_token_list()\n",
    "    for validation_idx, tokens in enumerate(validation_token_list):\n",
    "        sent = \" \".join(tokens)\n",
    "        doc:Doc = best_model_nlp(sent)\n",
    "\n",
    "        current_token_len = len(tokens)\n",
    "        current_ent_len = ents_to_len(doc.ents)\n",
    "        \n",
    "        if(current_token_len != current_ent_len):\n",
    "            if current_token_len > current_ent_len: # pad entities\n",
    "                span_list:list[EntSpan] = pad_ents(tokens, doc.ents)\n",
    "            else: # pad tokens\n",
    "                span_list:list[EntSpan] = [EntSpan(ent.text, ent.label_) for ent in doc.ents]\n",
    "                tokens:list[str] = pad_tokens(tokens, doc.ents)\n",
    "                print(validation_idx, \" Tokens Padded Successfully!\")\n",
    "            \n",
    "            if(len(tokens) != len(span_list)):\n",
    "                print(validation_idx, \" IDX does not match: \", current_token_len, \" | \", current_ent_len)\n",
    "            else:\n",
    "                print(validation_idx, \" IDX was padded to match! Span Len: \", len(span_list), \" | Tok Len: \", len(tokens))\n",
    "\n",
    "collection_to_predictions(validation_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens at IDX  25  DO NOT ALIGN!\n",
      "Expected Length:  40  | Predicted Length:  37\n",
      "40 _T_LOG:  ['members', 'of', 'the', 'plant', 'heme', 'activator', 'protein', '(', 'hap', ')', 'or', 'nuclear', 'factor', 'y', '(', 'nf', '-', 'y', ')', 'are', 'trimeric', 'transcription', 'factor', 'complexes', 'composed', 'of', 'the', 'nf', '-', 'ya', ',', 'nf', '-', 'yb', 'and', 'nf', '-', 'yc', 'subfamilies', '.']\n",
      "37 _E_LOG:  ['members', 'of', 'the', 'plant', 'heme', 'activator', 'protein', '(', 'hap', ')', 'or', 'nuclear', 'factor', 'y', '(', 'nf', 'y', ')', 'are', 'trimeric', 'transcription', 'factor', 'complexes', 'composed', 'of', 'the', 'nf', '-', 'ya', ',', 'nf', 'yb', 'and', 'nf', 'yc', 'subfamilies', '.']\n",
      "Tokens at IDX  32  DO NOT ALIGN!\n",
      "Expected Length:  50  | Predicted Length:  49\n",
      "50 _T_LOG:  ['ep', ':', 'group', ':', 'extra', '-', 'perigastric', 'lymph', 'node', 'group', 'a', 'recent', 'result', 'from', 's.', 'pombe', 'suggests', 'that', 'in', 'this', 'organism', ',', 'in', 'which', 'all', 'cos', 'are', 'mediated', 'by', 'mus81', '-', 'eme1', ',', 'the', 'predominant', 'meiotic', 'recombination', 'intermediate', 'is', 'a', 'single', 'holliday', 'junction', '(', 'shj', ')', '[', '26', ']', '.']\n",
      "49 _E_LOG:  ['ep', ':', 'group', ':', 'extra', 'perigastric', 'lymph', 'node', 'group', 'a', 'recent', 'result', 'from', 's.', 'pombe', 'suggests', 'that', 'in', 'this', 'organism', ',', 'in', 'which', 'all', 'cos', 'are', 'mediated', 'by', 'mus81', '-', 'eme1', ',', 'the', 'predominant', 'meiotic', 'recombination', 'intermediate', 'is', 'a', 'single', 'holliday', 'junction', '(', 'shj', ')', '[', '26', ']', '.']\n",
      "Tokens at IDX  47  DO NOT ALIGN!\n",
      "Expected Length:  67  | Predicted Length:  66\n",
      "67 _T_LOG:  ['the', 'simultaneous', 'loss', 'of', 'can1', 'and', 'ura3', '(', 'detected', 'on', 'media', 'containing', 'canavinine', '[', 'can', ']', 'and', '5', '-', 'fluoro', '-', 'orotic', 'acid', '[', '5', '-', 'foa', ']', ')', 'at', 'this', 'locus', 'occurs', 'at', 'a', 'rate', 'of', '8.9', '×', '10−8', '(', 'table', '1', ')', ',', 'approximately', '250', '-', 'fold', 'higher', 'than', 'the', 'rate', 'observed', 'at', 'chrv', '-', 'l', '(', '3.5', '×', '10−10', ';', 'table', '2', ')', '.']\n",
      "66 _E_LOG:  ['the', 'simultaneous', 'loss', 'of', 'can1', 'and', 'ura3', '(', 'detected', 'on', 'media', 'containing', 'canavinine', '[', 'can', ']', 'and', '5', '-', 'fluoro', '-', 'orotic', 'acid', '[', '5', 'foa', ']', ')', 'at', 'this', 'locus', 'occurs', 'at', 'a', 'rate', 'of', '8.9', '×', '10−8', '(', 'table', '1', ')', ',', 'approximately', '250', '-', 'fold', 'higher', 'than', 'the', 'rate', 'observed', 'at', 'chrv', '-', 'l', '(', '3.5', '×', '10−10', ';', 'table', '2', ')', '.']\n",
      "Tokens at IDX  68  DO NOT ALIGN!\n",
      "Expected Length:  20  | Predicted Length:  22\n",
      "20 _T_LOG:  ['”[6', ']', 'this', 'value', 'is', 'in', 'excellent', 'agreement', 'with', 'the', 'expected', 'molecular', 'weight', 'of', 'the', 'holoenzyme', ',', '28,705.88', 'da', '.']\n",
      "22 _E_LOG:  ['”', '[', '6', ']', 'this', 'value', 'is', 'in', 'excellent', 'agreement', 'with', 'the', 'expected', 'molecular', 'weight', 'of', 'the', 'holoenzyme', ',', '28,705.88', 'da', '.']\n",
      "Tokens at IDX  83  DO NOT ALIGN!\n",
      "Expected Length:  35  | Predicted Length:  34\n",
      "35 _T_LOG:  ['among', 'these', 'sirnas', ',', 'only', 'those', 'silencing', 'the', 'ca2', '+', '-independent', 'group', 'viiia', '(', 'gviiia)-pla2', 'inhibited', 'vsvg', 'transport', ',', 'resulting', 'in', 'accumulation', 'of', 'cargo', 'within', 'the', 'golgi', 'complex', '(', 'figure', '8l', 'and', '8p', ')', '.']\n",
      "34 _E_LOG:  ['among', 'these', 'sirnas', ',', 'only', 'those', 'silencing', 'the', 'ca2', '+', 'group', 'viiia', '(', 'gviiia)-pla2', 'inhibited', 'vsvg', 'transport', ',', 'resulting', 'in', 'accumulation', 'of', 'cargo', 'within', 'the', 'golgi', 'complex', '(', 'figure', '8l', 'and', '8p', ')', '.']\n",
      "Tokens at IDX  96  DO NOT ALIGN!\n",
      "Expected Length:  35  | Predicted Length:  33\n",
      "35 _T_LOG:  ['human', 'and', 'mouse', 'b', 'cells', 'were', 'activated', 'with', 'membrane', '-', 'tethered', 'or', 'soluble', 'mb', '-', 'fab′–anti', '-', 'ig', 'plus', 'streptavidin', 'before', 'staining', 'for', 'phosphorylated', 'wasp', 'wasp', '(', 'pwasp', ')', 'and', 'n-wasp', '(', 'pn', ')', '.']\n",
      "33 _E_LOG:  ['human', 'and', 'mouse', 'b', 'cells', 'were', 'activated', 'with', 'membrane', '-', 'tethered', 'or', 'soluble', 'mb', 'fab′–anti', 'ig', 'plus', 'streptavidin', 'before', 'staining', 'for', 'phosphorylated', 'wasp', 'wasp', '(', 'pwasp', ')', 'and', 'n-wasp', '(', 'pn', ')', '.']\n",
      "Tokens at IDX  110  DO NOT ALIGN!\n",
      "Expected Length:  8  | Predicted Length:  7\n",
      "8 _T_LOG:  ['tbc', ',', 'tre2', '-', 'bub2', '-', 'cdc16', '.']\n",
      "7 _E_LOG:  ['tbc', ',', 'tre2', '-', 'bub2', 'cdc16', '.']\n",
      "Tokens at IDX  124  DO NOT ALIGN!\n",
      "Expected Length:  37  | Predicted Length:  38\n",
      "37 _T_LOG:  ['additionally', ',', 'to', 'investigate', 'the', 'differences', 'between', 'sleep', 'and', 'n', 'animals', 'at', 'retrieval', ',', 'we', 'calculated', 'percentage', 'change', '(', 'sleep', '>', '+sdn', ')', 'on', 'each', 'plate', 'and', 'then', 'averaged', 'the', 'data', '(', 'see', 's7', 'fig', ')', '.']\n",
      "38 _E_LOG:  ['additionally', ',', 'to', 'investigate', 'the', 'differences', 'between', 'sleep', 'and', 'n', 'animals', 'at', 'retrieval', ',', 'we', 'calculated', 'percentage', 'change', '(', 'sleep', '>', '+', 'sdn', ')', 'on', 'each', 'plate', 'and', 'then', 'averaged', 'the', 'data', '(', 'see', 's7', 'fig', ')', '.']\n",
      "TOKEN LEN:  5000\n",
      "TAG LEN:  5000\n",
      "PREDICT TAG LEN:  4994\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'list' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 52\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPREDICT TAG LEN: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(predicted_tags))\n\u001b[0;32m     51\u001b[0m data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtoken_list\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     53\u001b[0m     data\u001b[38;5;241m.\u001b[39mappend([token_list[idx], true_tags[idx], predicted_tags[idx]])\n\u001b[0;32m     55\u001b[0m result_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTokens\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTags\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted Tags\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;31mTypeError\u001b[0m: 'list' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "# text = \" \".join(validation_collection.get_token_list()[0])\n",
    "# doc:Doc = best_model_nlp(text)\n",
    "\n",
    "predicted_tags = []\n",
    "validation_token_list = validation_collection.get_token_list()\n",
    "for validation_idx, tokens in enumerate(validation_token_list):\n",
    "    sent = \" \".join(tokens)\n",
    "    doc:Doc = best_model_nlp(sent)\n",
    "    tag_counter = 0\n",
    "    et = []\n",
    "    for ent in doc.ents:\n",
    "        if len(ent.text) > 1: # handle multiple tokens split\n",
    "            ent_tokens = ent.text.split(\" \")\n",
    "            if ent.label_ == \"LF\": # handle LF split\n",
    "                for idx, tok in enumerate(ent_tokens):\n",
    "                    if idx == 0:\n",
    "                        # print(str(tok)+\" | B-\"+str(ent.label_))\n",
    "                        predicted_tags.append(\"B-\"+str(ent.label_))\n",
    "                        tag_counter += 1\n",
    "                        et.append(str(tok))\n",
    "                    else:\n",
    "                        # print(str(tok)+\" | I-\"+str(ent.label_))\n",
    "                        predicted_tags.append(\"I-\"+str(ent.label_))\n",
    "                        tag_counter += 1\n",
    "                        et.append(str(tok))\n",
    "            else:\n",
    "                for tok in ent_tokens:\n",
    "                    # print(str(tok)+\" | B-\"+str(ent.label_))\n",
    "                    predicted_tags.append(\"B-\"+str(ent.label_))\n",
    "                    tag_counter += 1\n",
    "                    et.append(str(tok))\n",
    "        else:            \n",
    "            # print(str(ent.text)+\" | B-\"+str(ent.label_))\n",
    "            predicted_tags.append(\"B-\"+str(ent.label_))\n",
    "            tag_counter += 1\n",
    "            et.append(str(ent))\n",
    "    if len(tokens) != tag_counter:\n",
    "        print(\"Tokens at IDX \", validation_idx, \" DO NOT ALIGN!\")\n",
    "        print(\"Expected Length: \", len(tokens), \" | Predicted Length: \", tag_counter)\n",
    "        print(len(tokens), \"_T_LOG: \", tokens)\n",
    "        print(len(et), \"_E_LOG: \", et)\n",
    "    # break\n",
    "\n",
    "true_tags = flatten_list(validation_collection.get_ner_list())\n",
    "token_list = flatten_list(validation_collection.get_token_list())\n",
    "\n",
    "print(\"TOKEN LEN: \", len(token_list))\n",
    "print(\"TAG LEN: \", len(true_tags))\n",
    "print(\"PREDICT TAG LEN: \", len(predicted_tags))\n",
    "\n",
    "data = []\n",
    "for idx in range(token_list):\n",
    "    data.append([token_list[idx], true_tags[idx], predicted_tags[idx]])\n",
    "\n",
    "result_df = pd.DataFrame(data, columns=['Tokens', 'Tags', \"Predicted Tags\"])\n",
    "result_df.to_csv('result.csv', index=False)\n",
    "result_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
