{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parse dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset # Import dataset import function for hugging face\n",
    "dataset = load_dataset(\"surrey-nlp/PLOD-CW\") # import the coursework dataset from\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "import evaluate\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = dataset[\"train\"]\n",
    "train_tokens = train_dict[\"tokens\"]\n",
    "train_pos_tags = train_dict[\"pos_tags\"]\n",
    "train_ner_tags = train_dict[\"ner_tags\"]\n",
    "\n",
    "validation_dict = dataset[\"validation\"]\n",
    "validation_tokens = validation_dict[\"tokens\"]\n",
    "validation_pos_tags = validation_dict[\"pos_tags\"]\n",
    "validation_ner_tags = validation_dict[\"ner_tags\"]\n",
    "\n",
    "test_dict = dataset[\"test\"]\n",
    "test_tokens = test_dict[\"tokens\"]\n",
    "test_pos_tags = test_dict[\"pos_tags\"]\n",
    "test_ner_tags = test_dict[\"ner_tags\"]\n",
    "\n",
    "def data_to_lower(data:list[list[str]]) -> list[list[str]]:\n",
    "    return [[token.lower() for token in tokens] for tokens in data]\n",
    "\n",
    "train_tokens = data_to_lower(train_tokens)\n",
    "validation_tokens = data_to_lower(validation_tokens)\n",
    "test_tokens = data_to_lower(test_tokens)\n",
    "\n",
    "class DataItem:\n",
    "    def __init__(self, tokens, pos, ner, idx=0):\n",
    "        self.idx=idx\n",
    "        self.tokens:list[str] = tokens\n",
    "        self.pos:list[str] = pos\n",
    "        self.ner:list = ner\n",
    "        self.tokenised_inputs:list = tokenizer(self.tokens, is_split_into_words=True)\n",
    "\n",
    "    def get_as_tuple(self) -> tuple:\n",
    "        return (self.tokens, self.pos, self.ner)\n",
    "    \n",
    "    def get_as_tuple_list(self) -> list[tuple]:\n",
    "        tuple_list = []\n",
    "        for idx in range(len(self.tokens)-1):\n",
    "            tuple_list.append((self.tokens[idx], self.pos[idx], self.ner[idx]))\n",
    "        return tuple_list\n",
    "    \n",
    "    def ner_label2idx(self, label2idx_dict):\n",
    "        if not isinstance(self.ner[0], str):\n",
    "            print(\"WARNING - NER not listed as labels! NER Type: \",type(self.ner[0]),\", Exiting...\")\n",
    "            return\n",
    "        for idx, ner in enumerate(self.ner):\n",
    "            ner[idx] = label2idx_dict[ner]\n",
    "    \n",
    "    def ner_idx2label(self, idx2label_dict):\n",
    "        if not isinstance(self.ner[0], int):\n",
    "            print(\"WARNING - NER not listed as indecies! Exiting...\")\n",
    "            return\n",
    "        for idx, ner in enumerate(self.ner):\n",
    "            ner[idx] = idx2label_dict[ner]\n",
    "\n",
    "class DataCollection:\n",
    "    def __init__(self, data_collection:list[DataItem], max_token_length=512):\n",
    "        self.max_token_length = max_token_length\n",
    "        self.data_collection:list[DataItem] = data_collection\n",
    "        self.unique_tags = self.get_unique_tags()\n",
    "        self.item_embeddings:dict = self.create_item_embeddings(self.unique_tags)\n",
    "        self.reverse_embeddings:dict = {v:k for k,v in self.item_embeddings.items()}\n",
    "\n",
    "    def get_token_list(self) -> list[list[str]]:\n",
    "        return [data_item.tokens for data_item in self.data_collection]\n",
    "\n",
    "    def get_pos_list(self) -> list[list[str]]:\n",
    "        return [data_item.pos for data_item in self.data_collection]\n",
    "\n",
    "    def get_ner_list(self) -> list[list[str]]:\n",
    "        return [data_item.ner for data_item in self.data_collection]\n",
    "    \n",
    "    def get_ner_idx_list(self) -> list[list[str]]:\n",
    "        ner_idx_list_collection = []\n",
    "        for data_item in self.data_collection:\n",
    "            ner_idx_list = []\n",
    "            for ner_tag in data_item.ner:\n",
    "                ner_idx_list.append(self.item_embeddings[ner_tag])\n",
    "            ner_idx_list_collection.append(ner_idx_list)\n",
    "        return ner_idx_list_collection\n",
    "\n",
    "    def get_unique_tags(self) -> list[str]:\n",
    "        unique_list = []\n",
    "        ner_tags_list:list = self.get_ner_list()\n",
    "        for ner_list in ner_tags_list:\n",
    "            for ner in ner_list:\n",
    "                if ner not in unique_list:\n",
    "                    unique_list.append(ner)\n",
    "        return unique_list\n",
    "    \n",
    "    def create_item_embeddings(self, tags:list[str]) -> dict:\n",
    "        return {label:idx for idx, label in enumerate(tags)}\n",
    "    \n",
    "    def get_invalid_token_lengths(self) -> list[int]:\n",
    "        invalid_lengths = []\n",
    "        for idx, data_item in enumerate(self.data_collection):\n",
    "            if len(data_item.tokenised_inputs[\"input_ids\"]) >= self.max_token_length:\n",
    "                invalid_lengths.append(idx)\n",
    "                print(\"Data item idx \",idx,\" has tokens longer than \",self.max_token_length)\n",
    "        return invalid_lengths\n",
    "\n",
    "    def remove_invalid_token_length_items(self) -> None:\n",
    "        invalid_lengths = self.get_invalid_token_lengths()\n",
    "        for index in sorted(invalid_lengths, reverse=True):\n",
    "            del self.data_collection[index]\n",
    "\n",
    "train_data:list[DataItem] = []\n",
    "for idx in range(len(train_tokens)):\n",
    "    train_data.append(DataItem(train_tokens[idx], train_pos_tags[idx], train_ner_tags[idx], idx))\n",
    "train_collection:DataCollection = DataCollection(train_data)\n",
    "\n",
    "validation_data:list[DataItem] = []\n",
    "for idx in range(len(validation_tokens)):\n",
    "    validation_data.append(DataItem(validation_tokens[idx], validation_pos_tags[idx], validation_ner_tags[idx], idx))\n",
    "validation_collection:DataCollection = DataCollection(validation_data)\n",
    "\n",
    "test_data:list[DataItem] = []\n",
    "for idx in range(len(test_tokens)):\n",
    "    test_data.append(DataItem(test_tokens[idx], test_pos_tags[idx], test_ner_tags[idx], idx))\n",
    "test_collection:DataCollection = DataCollection(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data item idx  286  has tokens longer than  512\n",
      "IDX: 0\n",
      "IDX: 1\n",
      "IDX: 2\n",
      "IDX: 3\n",
      "IDX: 4\n",
      "IDX: 5\n",
      "IDX: 6\n",
      "IDX: 7\n",
      "IDX: 8\n",
      "IDX: 9\n",
      "IDX: 10\n",
      "IDX: 11\n",
      "IDX: 12\n",
      "IDX: 13\n",
      "IDX: 14\n",
      "IDX: 15\n",
      "IDX: 16\n",
      "IDX: 17\n",
      "IDX: 18\n",
      "IDX: 19\n",
      "IDX: 20\n",
      "IDX: 21\n",
      "IDX: 22\n",
      "IDX: 23\n",
      "IDX: 24\n",
      "IDX: 25\n",
      "IDX: 26\n",
      "IDX: 27\n",
      "IDX: 28\n",
      "IDX: 29\n",
      "IDX: 30\n",
      "IDX: 31\n",
      "IDX: 32\n",
      "IDX: 33\n",
      "IDX: 34\n",
      "IDX: 35\n",
      "IDX: 36\n",
      "IDX: 37\n",
      "IDX: 38\n",
      "IDX: 39\n",
      "IDX: 40\n",
      "IDX: 41\n",
      "IDX: 42\n",
      "IDX: 43\n",
      "IDX: 44\n",
      "IDX: 45\n",
      "IDX: 46\n",
      "IDX: 47\n",
      "IDX: 48\n",
      "IDX: 49\n",
      "IDX: 50\n",
      "IDX: 51\n",
      "IDX: 52\n",
      "IDX: 53\n",
      "IDX: 54\n",
      "IDX: 55\n",
      "IDX: 56\n",
      "IDX: 57\n",
      "IDX: 58\n",
      "IDX: 59\n",
      "IDX: 60\n",
      "IDX: 61\n",
      "IDX: 62\n",
      "IDX: 63\n",
      "IDX: 64\n",
      "IDX: 65\n",
      "IDX: 66\n",
      "IDX: 67\n",
      "IDX: 68\n",
      "IDX: 69\n",
      "IDX: 70\n",
      "IDX: 71\n",
      "IDX: 72\n",
      "IDX: 73\n",
      "IDX: 74\n",
      "IDX: 75\n",
      "IDX: 76\n",
      "IDX: 77\n",
      "IDX: 78\n",
      "IDX: 79\n",
      "IDX: 80\n",
      "IDX: 81\n",
      "IDX: 82\n",
      "IDX: 83\n",
      "IDX: 84\n",
      "IDX: 85\n",
      "IDX: 86\n",
      "IDX: 87\n",
      "IDX: 88\n",
      "IDX: 89\n",
      "IDX: 90\n",
      "IDX: 91\n",
      "IDX: 92\n",
      "IDX: 93\n",
      "IDX: 94\n",
      "IDX: 95\n",
      "IDX: 96\n",
      "IDX: 97\n",
      "IDX: 98\n",
      "IDX: 99\n",
      "IDX: 100\n",
      "IDX: 101\n",
      "IDX: 102\n",
      "IDX: 103\n",
      "IDX: 104\n",
      "IDX: 105\n",
      "IDX: 106\n",
      "IDX: 107\n",
      "IDX: 108\n",
      "IDX: 109\n",
      "IDX: 110\n",
      "IDX: 111\n",
      "IDX: 112\n",
      "IDX: 113\n",
      "IDX: 114\n",
      "IDX: 115\n",
      "IDX: 116\n",
      "IDX: 117\n",
      "IDX: 118\n",
      "IDX: 119\n",
      "IDX: 120\n",
      "IDX: 121\n",
      "IDX: 122\n",
      "IDX: 123\n",
      "IDX: 124\n",
      "IDX: 125\n",
      "IDX: 126\n",
      "IDX: 127\n",
      "IDX: 128\n",
      "IDX: 129\n",
      "IDX: 130\n",
      "IDX: 131\n",
      "IDX: 132\n",
      "IDX: 133\n",
      "IDX: 134\n",
      "IDX: 135\n",
      "IDX: 136\n",
      "IDX: 137\n",
      "IDX: 138\n",
      "IDX: 139\n",
      "IDX: 140\n",
      "IDX: 141\n",
      "IDX: 142\n",
      "IDX: 143\n",
      "IDX: 144\n",
      "IDX: 145\n",
      "IDX: 146\n",
      "IDX: 147\n",
      "IDX: 148\n",
      "IDX: 149\n",
      "IDX: 150\n",
      "IDX: 151\n",
      "IDX: 152\n",
      "IDX: 153\n",
      "IDX: 154\n",
      "IDX: 155\n",
      "IDX: 156\n",
      "IDX: 157\n",
      "IDX: 158\n",
      "IDX: 159\n",
      "IDX: 160\n",
      "IDX: 161\n",
      "IDX: 162\n",
      "IDX: 163\n",
      "IDX: 164\n",
      "IDX: 165\n",
      "IDX: 166\n",
      "IDX: 167\n",
      "IDX: 168\n",
      "IDX: 169\n",
      "IDX: 170\n",
      "IDX: 171\n",
      "IDX: 172\n",
      "IDX: 173\n",
      "IDX: 174\n",
      "IDX: 175\n",
      "IDX: 176\n",
      "IDX: 177\n",
      "IDX: 178\n",
      "IDX: 179\n",
      "IDX: 180\n",
      "IDX: 181\n",
      "IDX: 182\n",
      "IDX: 183\n",
      "IDX: 184\n",
      "IDX: 185\n",
      "IDX: 186\n",
      "IDX: 187\n",
      "IDX: 188\n",
      "IDX: 189\n",
      "IDX: 190\n",
      "IDX: 191\n",
      "IDX: 192\n",
      "IDX: 193\n",
      "IDX: 194\n",
      "IDX: 195\n",
      "IDX: 196\n",
      "IDX: 197\n",
      "IDX: 198\n",
      "IDX: 199\n",
      "IDX: 200\n",
      "IDX: 201\n",
      "IDX: 202\n",
      "IDX: 203\n",
      "IDX: 204\n",
      "IDX: 205\n",
      "IDX: 206\n",
      "IDX: 207\n",
      "IDX: 208\n",
      "IDX: 209\n",
      "IDX: 210\n",
      "IDX: 211\n",
      "IDX: 212\n",
      "IDX: 213\n",
      "IDX: 214\n",
      "IDX: 215\n",
      "IDX: 216\n",
      "IDX: 217\n",
      "IDX: 218\n",
      "IDX: 219\n",
      "IDX: 220\n",
      "IDX: 221\n",
      "IDX: 222\n",
      "IDX: 223\n",
      "IDX: 224\n",
      "IDX: 225\n",
      "IDX: 226\n",
      "IDX: 227\n",
      "IDX: 228\n",
      "IDX: 229\n",
      "IDX: 230\n",
      "IDX: 231\n",
      "IDX: 232\n",
      "IDX: 233\n",
      "IDX: 234\n",
      "IDX: 235\n",
      "IDX: 236\n",
      "IDX: 237\n",
      "IDX: 238\n",
      "IDX: 239\n",
      "IDX: 240\n",
      "IDX: 241\n",
      "IDX: 242\n",
      "IDX: 243\n",
      "IDX: 244\n",
      "IDX: 245\n",
      "IDX: 246\n",
      "IDX: 247\n",
      "IDX: 248\n",
      "IDX: 249\n",
      "IDX: 250\n",
      "IDX: 251\n",
      "IDX: 252\n",
      "IDX: 253\n",
      "IDX: 254\n",
      "IDX: 255\n",
      "IDX: 256\n",
      "IDX: 257\n",
      "IDX: 258\n",
      "IDX: 259\n",
      "IDX: 260\n",
      "IDX: 261\n",
      "IDX: 262\n",
      "IDX: 263\n",
      "IDX: 264\n",
      "IDX: 265\n",
      "IDX: 266\n",
      "IDX: 267\n",
      "IDX: 268\n",
      "IDX: 269\n",
      "IDX: 270\n",
      "IDX: 271\n",
      "IDX: 272\n",
      "IDX: 273\n",
      "IDX: 274\n",
      "IDX: 275\n",
      "IDX: 276\n",
      "IDX: 277\n",
      "IDX: 278\n",
      "IDX: 279\n",
      "IDX: 280\n",
      "IDX: 281\n",
      "IDX: 282\n",
      "IDX: 283\n",
      "IDX: 284\n",
      "IDX: 285\n",
      "IDX: 287\n",
      "IDX: 288\n",
      "IDX: 289\n",
      "IDX: 290\n",
      "IDX: 291\n",
      "IDX: 292\n",
      "IDX: 293\n",
      "IDX: 294\n",
      "IDX: 295\n",
      "IDX: 296\n",
      "IDX: 297\n",
      "IDX: 298\n",
      "IDX: 299\n",
      "IDX: 300\n",
      "IDX: 301\n",
      "IDX: 302\n",
      "IDX: 303\n",
      "IDX: 304\n",
      "IDX: 305\n",
      "IDX: 306\n",
      "IDX: 307\n",
      "IDX: 308\n",
      "IDX: 309\n",
      "IDX: 310\n",
      "IDX: 311\n",
      "IDX: 312\n",
      "IDX: 313\n",
      "IDX: 314\n",
      "IDX: 315\n",
      "IDX: 316\n",
      "IDX: 317\n",
      "IDX: 318\n",
      "IDX: 319\n",
      "IDX: 320\n",
      "IDX: 321\n",
      "IDX: 322\n",
      "IDX: 323\n",
      "IDX: 324\n",
      "IDX: 325\n",
      "IDX: 326\n",
      "IDX: 327\n",
      "IDX: 328\n",
      "IDX: 329\n",
      "IDX: 330\n",
      "IDX: 331\n",
      "IDX: 332\n",
      "IDX: 333\n",
      "IDX: 334\n",
      "IDX: 335\n",
      "IDX: 336\n",
      "IDX: 337\n",
      "IDX: 338\n",
      "IDX: 339\n",
      "IDX: 340\n",
      "IDX: 341\n",
      "IDX: 342\n",
      "IDX: 343\n",
      "IDX: 344\n",
      "IDX: 345\n",
      "IDX: 346\n",
      "IDX: 347\n",
      "IDX: 348\n",
      "IDX: 349\n",
      "IDX: 350\n",
      "IDX: 351\n",
      "IDX: 352\n",
      "IDX: 353\n",
      "IDX: 354\n",
      "IDX: 355\n",
      "IDX: 356\n",
      "IDX: 357\n",
      "IDX: 358\n",
      "IDX: 359\n",
      "IDX: 360\n",
      "IDX: 361\n",
      "IDX: 362\n",
      "IDX: 363\n",
      "IDX: 364\n",
      "IDX: 365\n",
      "IDX: 366\n",
      "IDX: 367\n",
      "IDX: 368\n",
      "IDX: 369\n",
      "IDX: 370\n",
      "IDX: 371\n",
      "IDX: 372\n",
      "IDX: 373\n",
      "IDX: 374\n",
      "IDX: 375\n",
      "IDX: 376\n",
      "IDX: 377\n",
      "IDX: 378\n",
      "IDX: 379\n",
      "IDX: 380\n",
      "IDX: 381\n",
      "IDX: 382\n",
      "IDX: 383\n",
      "IDX: 384\n",
      "IDX: 385\n",
      "IDX: 386\n",
      "IDX: 387\n",
      "IDX: 388\n",
      "IDX: 389\n",
      "IDX: 390\n",
      "IDX: 391\n",
      "IDX: 392\n",
      "IDX: 393\n",
      "IDX: 394\n",
      "IDX: 395\n",
      "IDX: 396\n",
      "IDX: 397\n",
      "IDX: 398\n",
      "IDX: 399\n",
      "IDX: 400\n",
      "IDX: 401\n",
      "IDX: 402\n",
      "IDX: 403\n",
      "IDX: 404\n",
      "IDX: 405\n",
      "IDX: 406\n",
      "IDX: 407\n",
      "IDX: 408\n",
      "IDX: 409\n",
      "IDX: 410\n",
      "IDX: 411\n",
      "IDX: 412\n",
      "IDX: 413\n",
      "IDX: 414\n",
      "IDX: 415\n",
      "IDX: 416\n",
      "IDX: 417\n",
      "IDX: 418\n",
      "IDX: 419\n",
      "IDX: 420\n",
      "IDX: 421\n",
      "IDX: 422\n",
      "IDX: 423\n",
      "IDX: 424\n",
      "IDX: 425\n",
      "IDX: 426\n",
      "IDX: 427\n",
      "IDX: 428\n",
      "IDX: 429\n",
      "IDX: 430\n",
      "IDX: 431\n",
      "IDX: 432\n",
      "IDX: 433\n",
      "IDX: 434\n",
      "IDX: 435\n",
      "IDX: 436\n",
      "IDX: 437\n",
      "IDX: 438\n",
      "IDX: 439\n",
      "IDX: 440\n",
      "IDX: 441\n",
      "IDX: 442\n",
      "IDX: 443\n",
      "IDX: 444\n",
      "IDX: 445\n",
      "IDX: 446\n",
      "IDX: 447\n",
      "IDX: 448\n",
      "IDX: 449\n",
      "IDX: 450\n",
      "IDX: 451\n",
      "IDX: 452\n",
      "IDX: 453\n",
      "IDX: 454\n",
      "IDX: 455\n",
      "IDX: 456\n",
      "IDX: 457\n",
      "IDX: 458\n",
      "IDX: 459\n",
      "IDX: 460\n",
      "IDX: 461\n",
      "IDX: 462\n",
      "IDX: 463\n",
      "IDX: 464\n",
      "IDX: 465\n",
      "IDX: 466\n",
      "IDX: 467\n",
      "IDX: 468\n",
      "IDX: 469\n",
      "IDX: 470\n",
      "IDX: 471\n",
      "IDX: 472\n",
      "IDX: 473\n",
      "IDX: 474\n",
      "IDX: 475\n",
      "IDX: 476\n",
      "IDX: 477\n",
      "IDX: 478\n",
      "IDX: 479\n",
      "IDX: 480\n",
      "IDX: 481\n",
      "IDX: 482\n",
      "IDX: 483\n",
      "IDX: 484\n",
      "IDX: 485\n",
      "IDX: 486\n",
      "IDX: 487\n",
      "IDX: 488\n",
      "IDX: 489\n",
      "IDX: 490\n",
      "IDX: 491\n",
      "IDX: 492\n",
      "IDX: 493\n",
      "IDX: 494\n",
      "IDX: 495\n",
      "IDX: 496\n",
      "IDX: 497\n",
      "IDX: 498\n",
      "IDX: 499\n",
      "IDX: 500\n",
      "IDX: 501\n",
      "IDX: 502\n",
      "IDX: 503\n",
      "IDX: 504\n",
      "IDX: 505\n",
      "IDX: 506\n",
      "IDX: 507\n",
      "IDX: 508\n",
      "IDX: 509\n",
      "IDX: 510\n",
      "IDX: 511\n",
      "IDX: 512\n",
      "IDX: 513\n",
      "IDX: 514\n",
      "IDX: 515\n",
      "IDX: 516\n",
      "IDX: 517\n",
      "IDX: 518\n",
      "IDX: 519\n",
      "IDX: 520\n",
      "IDX: 521\n",
      "IDX: 522\n",
      "IDX: 523\n",
      "IDX: 524\n",
      "IDX: 525\n",
      "IDX: 526\n",
      "IDX: 527\n",
      "IDX: 528\n",
      "IDX: 529\n",
      "IDX: 530\n",
      "IDX: 531\n",
      "IDX: 532\n",
      "IDX: 533\n",
      "IDX: 534\n",
      "IDX: 535\n",
      "IDX: 536\n",
      "IDX: 537\n",
      "IDX: 538\n",
      "IDX: 539\n",
      "IDX: 540\n",
      "IDX: 541\n",
      "IDX: 542\n",
      "IDX: 543\n",
      "IDX: 544\n",
      "IDX: 545\n",
      "IDX: 546\n",
      "IDX: 547\n",
      "IDX: 548\n",
      "IDX: 549\n",
      "IDX: 550\n",
      "IDX: 551\n",
      "IDX: 552\n",
      "IDX: 553\n",
      "IDX: 554\n",
      "IDX: 555\n",
      "IDX: 556\n",
      "IDX: 557\n",
      "IDX: 558\n",
      "IDX: 559\n",
      "IDX: 560\n",
      "IDX: 561\n",
      "IDX: 562\n",
      "IDX: 563\n",
      "IDX: 564\n",
      "IDX: 565\n",
      "IDX: 566\n",
      "IDX: 567\n",
      "IDX: 568\n",
      "IDX: 569\n",
      "IDX: 570\n",
      "IDX: 571\n",
      "IDX: 572\n",
      "IDX: 573\n",
      "IDX: 574\n",
      "IDX: 575\n",
      "IDX: 576\n",
      "IDX: 577\n",
      "IDX: 578\n",
      "IDX: 579\n",
      "IDX: 580\n",
      "IDX: 581\n",
      "IDX: 582\n",
      "IDX: 583\n",
      "IDX: 584\n",
      "IDX: 585\n",
      "IDX: 586\n",
      "IDX: 587\n",
      "IDX: 588\n",
      "IDX: 589\n",
      "IDX: 590\n",
      "IDX: 591\n",
      "IDX: 592\n",
      "IDX: 593\n",
      "IDX: 594\n",
      "IDX: 595\n",
      "IDX: 596\n",
      "IDX: 597\n",
      "IDX: 598\n",
      "IDX: 599\n",
      "IDX: 600\n",
      "IDX: 601\n",
      "IDX: 602\n",
      "IDX: 603\n",
      "IDX: 604\n",
      "IDX: 605\n",
      "IDX: 606\n",
      "IDX: 607\n",
      "IDX: 608\n",
      "IDX: 609\n",
      "IDX: 610\n",
      "IDX: 611\n",
      "IDX: 612\n",
      "IDX: 613\n",
      "IDX: 614\n",
      "IDX: 615\n",
      "IDX: 616\n",
      "IDX: 617\n",
      "IDX: 618\n",
      "IDX: 619\n",
      "IDX: 620\n",
      "IDX: 621\n",
      "IDX: 622\n",
      "IDX: 623\n",
      "IDX: 624\n",
      "IDX: 625\n",
      "IDX: 626\n",
      "IDX: 627\n",
      "IDX: 628\n",
      "IDX: 629\n",
      "IDX: 630\n",
      "IDX: 631\n",
      "IDX: 632\n",
      "IDX: 633\n",
      "IDX: 634\n",
      "IDX: 635\n",
      "IDX: 636\n",
      "IDX: 637\n",
      "IDX: 638\n",
      "IDX: 639\n",
      "IDX: 640\n",
      "IDX: 641\n",
      "IDX: 642\n",
      "IDX: 643\n",
      "IDX: 644\n",
      "IDX: 645\n",
      "IDX: 646\n",
      "IDX: 647\n",
      "IDX: 648\n",
      "IDX: 649\n",
      "IDX: 650\n",
      "IDX: 651\n",
      "IDX: 652\n",
      "IDX: 653\n",
      "IDX: 654\n",
      "IDX: 655\n",
      "IDX: 656\n",
      "IDX: 657\n",
      "IDX: 658\n",
      "IDX: 659\n",
      "IDX: 660\n",
      "IDX: 661\n",
      "IDX: 662\n",
      "IDX: 663\n",
      "IDX: 664\n",
      "IDX: 665\n",
      "IDX: 666\n",
      "IDX: 667\n",
      "IDX: 668\n",
      "IDX: 669\n",
      "IDX: 670\n",
      "IDX: 671\n",
      "IDX: 672\n",
      "IDX: 673\n",
      "IDX: 674\n",
      "IDX: 675\n",
      "IDX: 676\n",
      "IDX: 677\n",
      "IDX: 678\n",
      "IDX: 679\n",
      "IDX: 680\n",
      "IDX: 681\n",
      "IDX: 682\n",
      "IDX: 683\n",
      "IDX: 684\n",
      "IDX: 685\n",
      "IDX: 686\n",
      "IDX: 687\n",
      "IDX: 688\n",
      "IDX: 689\n",
      "IDX: 690\n",
      "IDX: 691\n",
      "IDX: 692\n",
      "IDX: 693\n",
      "IDX: 694\n",
      "IDX: 695\n",
      "IDX: 696\n",
      "IDX: 697\n",
      "IDX: 698\n",
      "IDX: 699\n",
      "IDX: 700\n",
      "IDX: 701\n",
      "IDX: 702\n",
      "IDX: 703\n",
      "IDX: 704\n",
      "IDX: 705\n",
      "IDX: 706\n",
      "IDX: 707\n",
      "IDX: 708\n",
      "IDX: 709\n",
      "IDX: 710\n",
      "IDX: 711\n",
      "IDX: 712\n",
      "IDX: 713\n",
      "IDX: 714\n",
      "IDX: 715\n",
      "IDX: 716\n",
      "IDX: 717\n",
      "IDX: 718\n",
      "IDX: 719\n",
      "IDX: 720\n",
      "IDX: 721\n",
      "IDX: 722\n",
      "IDX: 723\n",
      "IDX: 724\n",
      "IDX: 725\n",
      "IDX: 726\n",
      "IDX: 727\n",
      "IDX: 728\n",
      "IDX: 729\n",
      "IDX: 730\n",
      "IDX: 731\n",
      "IDX: 732\n",
      "IDX: 733\n",
      "IDX: 734\n",
      "IDX: 735\n",
      "IDX: 736\n",
      "IDX: 737\n",
      "IDX: 738\n",
      "IDX: 739\n",
      "IDX: 740\n",
      "IDX: 741\n",
      "IDX: 742\n",
      "IDX: 743\n",
      "IDX: 744\n",
      "IDX: 745\n",
      "IDX: 746\n",
      "IDX: 747\n",
      "IDX: 748\n",
      "IDX: 749\n",
      "IDX: 750\n",
      "IDX: 751\n",
      "IDX: 752\n",
      "IDX: 753\n",
      "IDX: 754\n",
      "IDX: 755\n",
      "IDX: 756\n",
      "IDX: 757\n",
      "IDX: 758\n",
      "IDX: 759\n",
      "IDX: 760\n",
      "IDX: 761\n",
      "IDX: 762\n",
      "IDX: 763\n",
      "IDX: 764\n",
      "IDX: 765\n",
      "IDX: 766\n",
      "IDX: 767\n",
      "IDX: 768\n",
      "IDX: 769\n",
      "IDX: 770\n",
      "IDX: 771\n",
      "IDX: 772\n",
      "IDX: 773\n",
      "IDX: 774\n",
      "IDX: 775\n",
      "IDX: 776\n",
      "IDX: 777\n",
      "IDX: 778\n",
      "IDX: 779\n",
      "IDX: 780\n",
      "IDX: 781\n",
      "IDX: 782\n",
      "IDX: 783\n",
      "IDX: 784\n",
      "IDX: 785\n",
      "IDX: 786\n",
      "IDX: 787\n",
      "IDX: 788\n",
      "IDX: 789\n",
      "IDX: 790\n",
      "IDX: 791\n",
      "IDX: 792\n",
      "IDX: 793\n",
      "IDX: 794\n",
      "IDX: 795\n",
      "IDX: 796\n",
      "IDX: 797\n",
      "IDX: 798\n",
      "IDX: 799\n",
      "IDX: 800\n",
      "IDX: 801\n",
      "IDX: 802\n",
      "IDX: 803\n",
      "IDX: 804\n",
      "IDX: 805\n",
      "IDX: 806\n",
      "IDX: 807\n",
      "IDX: 808\n",
      "IDX: 809\n",
      "IDX: 810\n",
      "IDX: 811\n",
      "IDX: 812\n",
      "IDX: 813\n",
      "IDX: 814\n",
      "IDX: 815\n",
      "IDX: 816\n",
      "IDX: 817\n",
      "IDX: 818\n",
      "IDX: 819\n",
      "IDX: 820\n",
      "IDX: 821\n",
      "IDX: 822\n",
      "IDX: 823\n",
      "IDX: 824\n",
      "IDX: 825\n",
      "IDX: 826\n",
      "IDX: 827\n",
      "IDX: 828\n",
      "IDX: 829\n",
      "IDX: 830\n",
      "IDX: 831\n",
      "IDX: 832\n",
      "IDX: 833\n",
      "IDX: 834\n",
      "IDX: 835\n",
      "IDX: 836\n",
      "IDX: 837\n",
      "IDX: 838\n",
      "IDX: 839\n",
      "IDX: 840\n",
      "IDX: 841\n",
      "IDX: 842\n",
      "IDX: 843\n",
      "IDX: 844\n",
      "IDX: 845\n",
      "IDX: 846\n",
      "IDX: 847\n",
      "IDX: 848\n",
      "IDX: 849\n",
      "IDX: 850\n",
      "IDX: 851\n",
      "IDX: 852\n",
      "IDX: 853\n",
      "IDX: 854\n",
      "IDX: 855\n",
      "IDX: 856\n",
      "IDX: 857\n",
      "IDX: 858\n",
      "IDX: 859\n",
      "IDX: 860\n",
      "IDX: 861\n",
      "IDX: 862\n",
      "IDX: 863\n",
      "IDX: 864\n",
      "IDX: 865\n",
      "IDX: 866\n",
      "IDX: 867\n",
      "IDX: 868\n",
      "IDX: 869\n",
      "IDX: 870\n",
      "IDX: 871\n",
      "IDX: 872\n",
      "IDX: 873\n",
      "IDX: 874\n",
      "IDX: 875\n",
      "IDX: 876\n",
      "IDX: 877\n",
      "IDX: 878\n",
      "IDX: 879\n",
      "IDX: 880\n",
      "IDX: 881\n",
      "IDX: 882\n",
      "IDX: 883\n",
      "IDX: 884\n",
      "IDX: 885\n",
      "IDX: 886\n",
      "IDX: 887\n",
      "IDX: 888\n",
      "IDX: 889\n",
      "IDX: 890\n",
      "IDX: 891\n",
      "IDX: 892\n",
      "IDX: 893\n",
      "IDX: 894\n",
      "IDX: 895\n",
      "IDX: 896\n",
      "IDX: 897\n",
      "IDX: 898\n",
      "IDX: 899\n",
      "IDX: 900\n",
      "IDX: 901\n",
      "IDX: 902\n",
      "IDX: 903\n",
      "IDX: 904\n",
      "IDX: 905\n",
      "IDX: 906\n",
      "IDX: 907\n",
      "IDX: 908\n",
      "IDX: 909\n",
      "IDX: 910\n",
      "IDX: 911\n",
      "IDX: 912\n",
      "IDX: 913\n",
      "IDX: 914\n",
      "IDX: 915\n",
      "IDX: 916\n",
      "IDX: 917\n",
      "IDX: 918\n",
      "IDX: 919\n",
      "IDX: 920\n",
      "IDX: 921\n",
      "IDX: 922\n",
      "IDX: 923\n",
      "IDX: 924\n",
      "IDX: 925\n",
      "IDX: 926\n",
      "IDX: 927\n",
      "IDX: 928\n",
      "IDX: 929\n",
      "IDX: 930\n",
      "IDX: 931\n",
      "IDX: 932\n",
      "IDX: 933\n",
      "IDX: 934\n",
      "IDX: 935\n",
      "IDX: 936\n",
      "IDX: 937\n",
      "IDX: 938\n",
      "IDX: 939\n",
      "IDX: 940\n",
      "IDX: 941\n",
      "IDX: 942\n",
      "IDX: 943\n",
      "IDX: 944\n",
      "IDX: 945\n",
      "IDX: 946\n",
      "IDX: 947\n",
      "IDX: 948\n",
      "IDX: 949\n",
      "IDX: 950\n",
      "IDX: 951\n",
      "IDX: 952\n",
      "IDX: 953\n",
      "IDX: 954\n",
      "IDX: 955\n",
      "IDX: 956\n",
      "IDX: 957\n",
      "IDX: 958\n",
      "IDX: 959\n",
      "IDX: 960\n",
      "IDX: 961\n",
      "IDX: 962\n",
      "IDX: 963\n",
      "IDX: 964\n",
      "IDX: 965\n",
      "IDX: 966\n",
      "IDX: 967\n",
      "IDX: 968\n",
      "IDX: 969\n",
      "IDX: 970\n",
      "IDX: 971\n",
      "IDX: 972\n",
      "IDX: 973\n",
      "IDX: 974\n",
      "IDX: 975\n",
      "IDX: 976\n",
      "IDX: 977\n",
      "IDX: 978\n",
      "IDX: 979\n",
      "IDX: 980\n",
      "IDX: 981\n",
      "IDX: 982\n",
      "IDX: 983\n",
      "IDX: 984\n",
      "IDX: 985\n",
      "IDX: 986\n",
      "IDX: 987\n",
      "IDX: 988\n",
      "IDX: 989\n",
      "IDX: 990\n",
      "IDX: 991\n",
      "IDX: 992\n",
      "IDX: 993\n",
      "IDX: 994\n",
      "IDX: 995\n",
      "IDX: 996\n",
      "IDX: 997\n",
      "IDX: 998\n",
      "IDX: 999\n",
      "IDX: 1000\n",
      "IDX: 1001\n",
      "IDX: 1002\n",
      "IDX: 1003\n",
      "IDX: 1004\n",
      "IDX: 1005\n",
      "IDX: 1006\n",
      "IDX: 1007\n",
      "IDX: 1008\n",
      "IDX: 1009\n",
      "IDX: 1010\n",
      "IDX: 1011\n",
      "IDX: 1012\n",
      "IDX: 1013\n",
      "IDX: 1014\n",
      "IDX: 1015\n",
      "IDX: 1016\n",
      "IDX: 1017\n",
      "IDX: 1018\n",
      "IDX: 1019\n",
      "IDX: 1020\n",
      "IDX: 1021\n",
      "IDX: 1022\n",
      "IDX: 1023\n",
      "IDX: 1024\n",
      "IDX: 1025\n",
      "IDX: 1026\n",
      "IDX: 1027\n",
      "IDX: 1028\n",
      "IDX: 1029\n",
      "IDX: 1030\n",
      "IDX: 1031\n",
      "IDX: 1032\n",
      "IDX: 1033\n",
      "IDX: 1034\n",
      "IDX: 1035\n",
      "IDX: 1036\n",
      "IDX: 1037\n",
      "IDX: 1038\n",
      "IDX: 1039\n",
      "IDX: 1040\n",
      "IDX: 1041\n",
      "IDX: 1042\n",
      "IDX: 1043\n",
      "IDX: 1044\n",
      "IDX: 1045\n",
      "IDX: 1046\n",
      "IDX: 1047\n",
      "IDX: 1048\n",
      "IDX: 1049\n",
      "IDX: 1050\n",
      "IDX: 1051\n",
      "IDX: 1052\n",
      "IDX: 1053\n",
      "IDX: 1054\n",
      "IDX: 1055\n",
      "IDX: 1056\n",
      "IDX: 1057\n",
      "IDX: 1058\n",
      "IDX: 1059\n",
      "IDX: 1060\n",
      "IDX: 1061\n",
      "IDX: 1062\n",
      "IDX: 1063\n",
      "IDX: 1064\n",
      "IDX: 1065\n",
      "IDX: 1066\n",
      "IDX: 1067\n",
      "IDX: 1068\n",
      "IDX: 1069\n",
      "IDX: 1070\n",
      "IDX: 1071\n"
     ]
    }
   ],
   "source": [
    "train_collection.remove_invalid_token_length_items()\n",
    "validation_collection.remove_invalid_token_length_items()\n",
    "test_collection.remove_invalid_token_length_items()\n",
    "\n",
    "for data_item in train_collection.data_collection:\n",
    "    print(\"IDX:\", data_item.idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following https://huggingface.co/docs/transformers/main/en/tasks/token_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create label2id functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B-LF': 0, 'B-AC': 1, 'B-O': 2, 'I-LF': 3}\n",
      "B-O\t2\t\n",
      "B-O\t2\t\n",
      "B-O\t2\t\n",
      "B-O\t2\t\n",
      "B-LF\t0\t\n",
      "I-LF\t3\t\n",
      "I-LF\t3\t\n",
      "I-LF\t3\t\n",
      "I-LF\t3\t\n",
      "B-O\t2\t\n",
      "B-AC\t1\t\n",
      "B-O\t2\t\n",
      "B-O\t2\t\n",
      "B-O\t2\t\n",
      "B-O\t2\t\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['B-O',\n",
       " 'B-O',\n",
       " 'B-O',\n",
       " 'B-O',\n",
       " 'B-LF',\n",
       " 'I-LF',\n",
       " 'I-LF',\n",
       " 'I-LF',\n",
       " 'I-LF',\n",
       " 'B-O',\n",
       " 'B-AC',\n",
       " 'B-O',\n",
       " 'B-O',\n",
       " 'B-O',\n",
       " 'B-O']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def print_lists_side_by_side(a_list, b_list):\n",
    "    for i in range(max(len(b_list), len(a_list))):\n",
    "        try:\n",
    "            print(f\"{a_list[i]}\\t\", end=\"\")\n",
    "        except IndexError:\n",
    "            print(f\" \\t\", end=\"\")\n",
    "        try:\n",
    "            print(f\"{b_list[i]}\\t\")\n",
    "        except IndexError:\n",
    "            print(f\" \\t\")\n",
    "\n",
    "def get_unique_labels(data:list[DataItem]) -> list[str]:\n",
    "    return list(set([ner for item in data for ner in item.ner]))\n",
    "    # return [\n",
    "    #     ner # returned item to list\n",
    "    #     for item in data # original list to item\n",
    "    #     for ner in item.ner # inner list to out list\n",
    "    # ]\n",
    "\n",
    "unique_labels = get_unique_labels(train_data)\n",
    "\n",
    "def create_label_index(labels:list) -> dict:\n",
    "    return {label:idx for idx, label in enumerate(labels)}\n",
    "\n",
    "label_index = create_label_index(unique_labels)\n",
    "label_index\n",
    "\n",
    "def labels2ids(labels:list[str], label_index:dict) -> list[int]:\n",
    "    id_list:list[int] = []\n",
    "    for label in labels:\n",
    "        if label in label_index.keys():\n",
    "            id_list.append(label_index[label])\n",
    "        else:\n",
    "            id_list.append(None)\n",
    "    return id_list\n",
    "\n",
    "def ids2labels(ids:list[int], label_index:dict) -> list[str]:\n",
    "    label_list:list[int] = []\n",
    "    for id in ids:\n",
    "        label_list.append(list(label_index.keys())[list(label_index.values()).index(id)])\n",
    "    return label_list\n",
    "\n",
    "print(label_index)\n",
    "ner_tags_test = train_data[0].ner\n",
    "ner_to_ids_test = labels2ids(train_data[0].ner, label_index)\n",
    "print_lists_side_by_side(ner_tags_test, ner_to_ids_test)\n",
    "ids_to_ner_test = ids2labels(ner_to_ids_test, label_index)\n",
    "ids_to_ner_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization Example\n",
    "\n",
    "Adds extra start and end tags (CLS and SEP), as well potentially splits one word into 2. Thus have to realign indecies.\n",
    "\n",
    "We also have to assign -100 to CLS and SEP so they are ignored by PyTorch loss function (CrossEntropyLoss)\n",
    "\n",
    "Only label first token of a word, add -100 for subtokens of the same word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'for', 'this', 'purpose', 'the', 'gothenburg', 'young', 'persons', 'empowerment', 'scale', '(', 'g', '##ype', '##s', ')', 'was', 'developed', '.', '[SEP]']\n",
      "{'input_ids': [101, 2005, 2023, 3800, 1996, 22836, 2402, 5381, 23011, 4094, 1006, 1043, 18863, 2015, 1007, 2001, 2764, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "example = dataset[\"train\"][0]\n",
    "tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "print(tokens)\n",
    "print(tokenized_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "def tokenize_and_align_labels(data_collection:DataCollection):\n",
    "    tokenized_inputs = tokenizer(data_collection.get_token_list(), truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(data_collection.get_ner_idx_list()):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_train = tokenize_and_align_labels(train_collection)\n",
    "tokenized_validation = tokenize_and_align_labels(validation_collection)\n",
    "tokenized_test = tokenize_and_align_labels(test_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = train_collection.unique_tags\n",
    "labels = train_collection.unique_tags\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "id2label = train_collection.reverse_embeddings\n",
    "label2id = train_collection.item_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"distilbert/distilbert-base-uncased\", num_labels=len(labels), id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_dict_to_list_of_dict(d):\n",
    "    new_list = []\n",
    "\n",
    "    for labels, inputs in zip(d[\"labels\"], d[\"input_ids\"]):\n",
    "        entry = {\"input_ids\": inputs, \"labels\": labels}\n",
    "        new_list.append(entry)\n",
    "\n",
    "    return new_list\n",
    "\n",
    "tokenised_train = turn_dict_to_list_of_dict(tokenized_train)\n",
    "tokenised_val = turn_dict_to_list_of_dict(tokenized_validation)\n",
    "tokenised_test = turn_dict_to_list_of_dict(tokenized_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olive\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\accelerate\\accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daef24e87d08429ab80d7beea0165fdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/134 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5130072a0bfc4e50974a24eb82edba12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5216518044471741, 'eval_precision': 0.8701026608003353, 'eval_recall': 0.8307661532306462, 'eval_f1': 0.8499795333606223, 'eval_accuracy': 0.8422, 'eval_runtime': 0.2627, 'eval_samples_per_second': 582.502, 'eval_steps_per_second': 38.072, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "166b48e497554ebea8ebb1444b746f4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6062090396881104, 'eval_precision': 0.8744166313109886, 'eval_recall': 0.8245649129825965, 'eval_f1': 0.8487593946257592, 'eval_accuracy': 0.8254, 'eval_runtime': 0.1861, 'eval_samples_per_second': 822.22, 'eval_steps_per_second': 53.74, 'epoch': 2.0}\n",
      "{'train_runtime': 8.4355, 'train_samples_per_second': 253.925, 'train_steps_per_second': 15.885, 'train_loss': 0.3820226014550053, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=134, training_loss=0.3820226014550053, metrics={'train_runtime': 8.4355, 'train_samples_per_second': 253.925, 'train_steps_per_second': 15.885, 'train_loss': 0.3820226014550053, 'epoch': 2.0})"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"distilbert_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=['none'], # REQUIRED because otherwise keeps asking to log into \"wandb\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenised_train,\n",
    "    eval_dataset=tokenised_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
