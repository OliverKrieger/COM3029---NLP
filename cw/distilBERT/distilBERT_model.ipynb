{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parse dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olive\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "C:\\Users\\olive\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "C:\\Users\\olive\\AppData\\Local\\Temp\\ipykernel_38968\\2897015339.py:11: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"seqeval\")\n",
      "C:\\Users\\olive\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\datasets\\load.py:756: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric # Import dataset import function for hugging face\n",
    "dataset = load_dataset(\"surrey-nlp/PLOD-CW\") # import the coursework dataset from\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification, pipeline\n",
    "from transformers import BatchEncoding\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "import evaluate\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "metric = load_metric(\"seqeval\")\n",
    "\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"ner\" # Should be one of \"ner\", \"pos\" or \"chunk\"\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(', 'EGF', ',', 'epidermal', 'growth', 'factor', ';', 'TGF', ',', 'transforming', 'growth', 'factor', ';', 'BTC', ',', 'betacellulin', ';', 'HB', '-', 'EGF', ',', 'heparin', '-', 'binding', 'epidermal', 'growth', 'factor', '(', 'EGF)-like', 'growth', 'factor', ';', 'EREG', ',', 'epiregulin', ';', 'NRG1', ',', 'neuregulin-1', ';', 'NRG2', ',', 'neuregulin-2', ';', 'NRG3', ',', 'neuregulin-3', ';', 'NRG4', ',', 'neuregulin-4', ';', 'PLCÎ³', ',', 'phospholipase', 'C', 'type', 'gamma', ';', 'CAMK2B', ',', 'calcium', '/', 'calmodulin', 'dependent', 'protein', 'kinase', ';', 'PRKCB', ',', 'Protein', 'kinase', 'C', '-', 'beta', ';', 'STAT5', ',', 'Signal', 'transducer', 'and', 'activator', 'of', 'transcription', '5', ';', 'src', ',', 'Rous', 'sarcoma', 'virus', 'gene', ';', 'CRK', ',', 'C', 'T10', 'regulator', 'of', 'a', 'tyrosine', 'kinase', ';', 'NCL', ',', 'NCK', 'Adaptor', 'Protein', '2', ';', 'PTK2', ',', 'PTK2', 'protein', 'tyrosine', 'kinase', '2', ';', 'ABL2', ',', 'V', '-', 'Abl', 'Abelson', 'Murine', 'Leukemia', 'Viral', 'Oncogene', 'Homolog', '2', ';', 'PAK2', ',', 'P21', '(', 'RAC1', ')', 'Activated', 'Kinase', '2', ';', 'MAP2K4', ',', 'Mitogen', '-', 'Activated', 'Protein', 'Kinase', 'Kinase', '4', ';', 'MAPK10', ',', 'Mitogen', '-', 'Activated', 'Protein', 'Kinase', '10', ';', 'SOS1', ',', 'SOS', 'Ras', '/', 'Rac', 'Guanine', 'Nucleotide', 'Exchange', 'Factor', '1', ';', 'Grb2', ',', 'Growth', 'Factor', 'Receptor', 'Bound', 'Protein', '2', ';', 'SHC4', ',', 'Src', 'Homology', '2', 'Domain', '-', 'Containing', '-', 'Transforming', 'Protein', 'C4', ';', 'PIK3C4', ',', 'Phosphatidylinositol-4,5', '-', 'Bisphosphate', '3', '-', 'Kinase', 'Catalytic', 'Subunit', ';', 'AKT3', ',', 'KT', 'Serine', '/', 'Threonine', 'Kinase', '3', ';', 'mTOR', ',', 'Mechanistic', 'Target', 'Of', 'Rapamycin', 'Kinase', ';', 'BCL2', ',', 'BCL2', 'Associated', 'Agonist', 'Of', 'Cell', 'Death', ';', 'GSK3B', ',', 'Glycogen', 'Synthase', 'Kinase', '3', 'Beta', ';', 'CDKN1A', ',', 'Cyclin', 'Dependent', 'Kinase', 'Inhibitor', '1A', ';', 'EIF4EBP1', ',', 'Eukaryotic', 'Translation', 'Initiation', 'Factor', '4E', 'Binding', 'Protein', '1', ';', 'BRAF', ',', 'B', '-', 'Raf', 'Proto', '-', 'Oncogene', ',', 'Serine', '/', 'Threonine', 'Kinase', ';', 'RPS6KB1', ',', 'Ribosomal', 'Protein', 'S6', 'Kinase', 'B1', ';', 'KRAS', ',', 'KRAS', 'Proto', '-', 'Oncogene', ',', 'GTPase', ';', 'JUN', ',', 'Jun', 'Proto', '-', 'Oncogene', ',', 'AP-1', 'Transcription', 'Factor', 'Subunit', ';', 'ELK', ',', 'ETS', 'Transcription', 'Factor', ';', 'Myc', ',', 'MYC', 'Proto', '-', 'Oncogene', ',', 'BHLH', 'Transcription', 'Factor', ';', 'ER', ',', 'endoplasmic', 'reticulum', '.']\n",
      "['[CLS]', '(', 'e', '##gf', ',', 'ep', '##ider', '##mal', 'growth', 'factor', ';', 't', '##gf', ',', 'transforming', 'growth', 'factor', ';', 'bt', '##c', ',', 'beta', '##cel', '##lu', '##lin', ';', 'h', '##b', '-', 'e', '##gf', ',', 'he', '##par', '##in', '-', 'binding', 'ep', '##ider', '##mal', 'growth', 'factor', '(', 'e', '##gf', ')', '-', 'like', 'growth', 'factor', ';', 'er', '##eg', ',', 'ep', '##ire', '##gul', '##in', ';', 'nr', '##g', '##1', ',', 'ne', '##ure', '##gul', '##in', '-', '1', ';', 'nr', '##g', '##2', ',', 'ne', '##ure', '##gul', '##in', '-', '2', ';', 'nr', '##g', '##3', ',', 'ne', '##ure', '##gul', '##in', '-', '3', ';', 'nr', '##g', '##4', ',', 'ne', '##ure', '##gul', '##in', '-', '4', ';', 'plc', '##Î³', ',', 'ph', '##os', '##ph', '##oli', '##pas', '##e', 'c', 'type', 'gamma', ';', 'cam', '##k', '##2', '##b', ',', 'calcium', '/', 'calm', '##od', '##ulin', 'dependent', 'protein', 'kinase', ';', 'pr', '##k', '##cb', ',', 'protein', 'kinase', 'c', '-', 'beta', ';', 'stat', '##5', ',', 'signal', 'trans', '##du', '##cer', 'and', 'act', '##iva', '##tor', 'of', 'transcription', '5', ';', 'sr', '##c', ',', 'ro', '##us', 'sar', '##com', '##a', 'virus', 'gene', ';', 'cr', '##k', ',', 'c', 't', '##10', 'regulator', 'of', 'a', 'ty', '##ros', '##ine', 'kinase', ';', 'nc', '##l', ',', 'nc', '##k', 'adapt', '##or', 'protein', '2', ';', 'pt', '##k', '##2', ',', 'pt', '##k', '##2', 'protein', 'ty', '##ros', '##ine', 'kinase', '2', ';', 'ab', '##l', '##2', ',', 'v', '-', 'ab', '##l', 'abel', '##son', 'mu', '##rine', 'leukemia', 'viral', 'on', '##co', '##gen', '##e', 'homo', '##log', '2', ';', 'pak', '##2', ',', 'p', '##21', '(', 'ra', '##c', '##1', ')', 'activated', 'kinase', '2', ';', 'map', '##2', '##k', '##4', ',', 'mit', '##ogen', '-', 'activated', 'protein', 'kinase', 'kinase', '4', ';', 'map', '##k', '##10', ',', 'mit', '##ogen', '-', 'activated', 'protein', 'kinase', '10', ';', 'so', '##s', '##1', ',', 'so', '##s', 'ras', '/', 'ra', '##c', 'gu', '##ani', '##ne', 'nu', '##cle', '##otide', 'exchange', 'factor', '1', ';', 'gr', '##b', '##2', ',', 'growth', 'factor', 'receptor', 'bound', 'protein', '2', ';', 'sh', '##c', '##4', ',', 'sr', '##c', 'homo', '##logy', '2', 'domain', '-', 'containing', '-', 'transforming', 'protein', 'c', '##4', ';', 'pi', '##k', '##3', '##c', '##4', ',', 'ph', '##os', '##pha', '##ti', '##dy', '##lino', '##sit', '##ol', '-', '4', ',', '5', '-', 'bis', '##ph', '##os', '##phate', '3', '-', 'kinase', 'catalytic', 'subunit', ';', 'ak', '##t', '##3', ',', 'k', '##t', 'ser', '##ine', '/', 'th', '##re', '##oni', '##ne', 'kinase', '3', ';', 'mt', '##or', ',', 'me', '##chan', '##istic', 'target', 'of', 'rap', '##amy', '##cin', 'kinase', ';', 'bc', '##l', '##2', ',', 'bc', '##l', '##2', 'associated', 'ago', '##nist', 'of', 'cell', 'death', ';', 'gs', '##k', '##3', '##b', ',', 'g', '##ly', '##co', '##gen', 'synth', '##ase', 'kinase', '3', 'beta', ';', 'cd', '##k', '##n', '##1', '##a', ',', 'cy', '##cl', '##in', 'dependent', 'kinase', 'inhibitor', '1a', ';', 'e', '##if', '##4', '##eb', '##p', '##1', ',', 'eu', '##kar', '##yo', '##tic', 'translation', 'initiation', 'factor', '4', '##e', 'binding', 'protein', '1', ';', 'bra', '##f', ',', 'b', '-', 'raf', 'proto', '-', 'on', '##co', '##gen', '##e', ',', 'ser', '##ine', '/', 'th', '##re', '##oni', '##ne', 'kinase', ';', 'r', '##ps', '##6', '##k', '##b', '##1', ',', 'rib', '##osomal', 'protein', 's', '##6', 'kinase', 'b1', ';', 'k', '##ras', ',', 'k', '##ras', 'proto', '-', 'on', '##co', '##gen', '##e', ',', 'gt', '##pas', '##e', ';', 'jun', ',', 'jun', 'proto', '-', 'on', '##co', '##gen', '##e', ',', 'ap', '-', '1', 'transcription', 'factor', 'subunit', ';', 'elk', ',', 'et', '##s', 'transcription', 'factor', ';', 'my', '##c', ',', 'my', '##c', 'proto', '-', 'on', '##co', '##gen', '##e', ',', 'b', '##hl', '##h', 'transcription', 'factor', ';', 'er', ',', 'end', '##op', '##las', '##mic', 're', '##tic', '##ulum', '.', '[SEP]']\n",
      "[101, 1006, 1041, 25708, 1010, 4958, 18688, 9067, 3930, 5387, 1025, 1056, 25708, 1010, 17903, 3930, 5387, 1025, 18411, 2278, 1010, 8247, 29109, 7630, 4115, 1025, 1044, 2497, 1011, 1041, 25708, 1010, 2002, 19362, 2378, 1011, 8031, 4958, 18688, 9067, 3930, 5387, 1006, 1041, 25708, 1007, 1011, 2066, 3930, 5387, 1025, 9413, 13910, 1010, 4958, 7442, 24848, 2378, 1025, 17212, 2290, 2487, 1010, 11265, 5397, 24848, 2378, 1011, 1015, 1025, 17212, 2290, 2475, 1010, 11265, 5397, 24848, 2378, 1011, 1016, 1025, 17212, 2290, 2509, 1010, 11265, 5397, 24848, 2378, 1011, 1017, 1025, 17212, 2290, 2549, 1010, 11265, 5397, 24848, 2378, 1011, 1018, 1025, 15492, 29721, 1010, 6887, 2891, 8458, 10893, 19707, 2063, 1039, 2828, 13091, 1025, 11503, 2243, 2475, 2497, 1010, 13853, 1013, 5475, 7716, 18639, 7790, 5250, 21903, 1025, 10975, 2243, 27421, 1010, 5250, 21903, 1039, 1011, 8247, 1025, 28093, 2629, 1010, 4742, 9099, 8566, 17119, 1998, 2552, 11444, 4263, 1997, 14193, 1019, 1025, 5034, 2278, 1010, 20996, 2271, 18906, 9006, 2050, 7865, 4962, 1025, 13675, 2243, 1010, 1039, 1056, 10790, 21618, 1997, 1037, 5939, 7352, 3170, 21903, 1025, 13316, 2140, 1010, 13316, 2243, 15581, 2953, 5250, 1016, 1025, 13866, 2243, 2475, 1010, 13866, 2243, 2475, 5250, 5939, 7352, 3170, 21903, 1016, 1025, 11113, 2140, 2475, 1010, 1058, 1011, 11113, 2140, 16768, 3385, 14163, 11467, 25468, 13434, 2006, 3597, 6914, 2063, 24004, 21197, 1016, 1025, 22190, 2475, 1010, 1052, 17465, 1006, 10958, 2278, 2487, 1007, 8878, 21903, 1016, 1025, 4949, 2475, 2243, 2549, 1010, 10210, 23924, 1011, 8878, 5250, 21903, 21903, 1018, 1025, 4949, 2243, 10790, 1010, 10210, 23924, 1011, 8878, 5250, 21903, 2184, 1025, 2061, 2015, 2487, 1010, 2061, 2015, 20710, 1013, 10958, 2278, 19739, 7088, 2638, 16371, 14321, 26601, 3863, 5387, 1015, 1025, 24665, 2497, 2475, 1010, 3930, 5387, 10769, 5391, 5250, 1016, 1025, 14021, 2278, 2549, 1010, 5034, 2278, 24004, 6483, 1016, 5884, 1011, 4820, 1011, 17903, 5250, 1039, 2549, 1025, 14255, 2243, 2509, 2278, 2549, 1010, 6887, 2891, 21890, 3775, 5149, 25226, 28032, 4747, 1011, 1018, 1010, 1019, 1011, 20377, 8458, 2891, 24556, 1017, 1011, 21903, 26244, 24312, 1025, 17712, 2102, 2509, 1010, 1047, 2102, 14262, 3170, 1013, 16215, 2890, 10698, 2638, 21903, 1017, 1025, 11047, 2953, 1010, 2033, 14856, 6553, 4539, 1997, 9680, 24079, 15459, 21903, 1025, 4647, 2140, 2475, 1010, 4647, 2140, 2475, 3378, 3283, 26942, 1997, 3526, 2331, 1025, 28177, 2243, 2509, 2497, 1010, 1043, 2135, 3597, 6914, 24203, 11022, 21903, 1017, 8247, 1025, 3729, 2243, 2078, 2487, 2050, 1010, 22330, 20464, 2378, 7790, 21903, 24054, 20720, 1025, 1041, 10128, 2549, 15878, 2361, 2487, 1010, 7327, 6673, 7677, 4588, 5449, 17890, 5387, 1018, 2063, 8031, 5250, 1015, 1025, 11655, 2546, 1010, 1038, 1011, 7148, 15053, 1011, 2006, 3597, 6914, 2063, 1010, 14262, 3170, 1013, 16215, 2890, 10698, 2638, 21903, 1025, 1054, 4523, 2575, 2243, 2497, 2487, 1010, 19395, 27642, 5250, 1055, 2575, 21903, 29491, 1025, 1047, 8180, 1010, 1047, 8180, 15053, 1011, 2006, 3597, 6914, 2063, 1010, 14181, 19707, 2063, 1025, 12022, 1010, 12022, 15053, 1011, 2006, 3597, 6914, 2063, 1010, 9706, 1011, 1015, 14193, 5387, 24312, 1025, 18995, 1010, 3802, 2015, 14193, 5387, 1025, 2026, 2278, 1010, 2026, 2278, 15053, 1011, 2006, 3597, 6914, 2063, 1010, 1038, 7317, 2232, 14193, 5387, 1025, 9413, 1010, 2203, 7361, 8523, 7712, 2128, 4588, 25100, 1012, 102]\n",
      "542\n"
     ]
    }
   ],
   "source": [
    "v = dataset[\"train\"][286:287][\"tokens\"][0]\n",
    "input_tokens = tokenizer(v, is_split_into_words=True)\n",
    "input_ids = input_tokens[\"input_ids\"]\n",
    "id_tokens = []\n",
    "for token in input_ids:\n",
    "    id_tokens.append(tokenizer.convert_ids_to_tokens(token))\n",
    "print(v)\n",
    "print(id_tokens)\n",
    "print(input_ids)\n",
    "print(len(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = dataset[\"train\"]\n",
    "train_tokens = train_dict[\"tokens\"]\n",
    "train_pos_tags = train_dict[\"pos_tags\"]\n",
    "train_ner_tags = train_dict[\"ner_tags\"]\n",
    "\n",
    "validation_dict = dataset[\"validation\"]\n",
    "validation_tokens = validation_dict[\"tokens\"]\n",
    "validation_pos_tags = validation_dict[\"pos_tags\"]\n",
    "validation_ner_tags = validation_dict[\"ner_tags\"]\n",
    "\n",
    "test_dict = dataset[\"test\"]\n",
    "test_tokens = test_dict[\"tokens\"]\n",
    "test_pos_tags = test_dict[\"pos_tags\"]\n",
    "test_ner_tags = test_dict[\"ner_tags\"]\n",
    "\n",
    "def data_to_lower(data:list[list[str]]) -> list[list[str]]:\n",
    "    return [[token.lower() for token in tokens] for tokens in data]\n",
    "\n",
    "train_tokens = data_to_lower(train_tokens)\n",
    "validation_tokens = data_to_lower(validation_tokens)\n",
    "test_tokens = data_to_lower(test_tokens)\n",
    "\n",
    "class DataItem:\n",
    "    def __init__(self, tokens, pos, ner, idx=0):\n",
    "        self.idx=idx\n",
    "        self.tokens:list[str] = tokens\n",
    "        self.pos:list[str] = pos\n",
    "        self.ner:list = ner\n",
    "        self.tokenised_inputs:BatchEncoding = tokenizer(self.tokens, is_split_into_words=True) # also contains attention mask!\n",
    "\n",
    "    def get_as_tuple(self) -> tuple:\n",
    "        return (self.tokens, self.pos, self.ner)\n",
    "    \n",
    "    def get_as_tuple_list(self) -> list[tuple]:\n",
    "        tuple_list = []\n",
    "        for idx in range(len(self.tokens)-1):\n",
    "            tuple_list.append((self.tokens[idx], self.pos[idx], self.ner[idx]))\n",
    "        return tuple_list\n",
    "    \n",
    "    def ner_label2idx(self, label2idx_dict):\n",
    "        if not isinstance(self.ner[0], str):\n",
    "            print(\"WARNING - NER not listed as labels! NER Type: \",type(self.ner[0]),\", Exiting...\")\n",
    "            return\n",
    "        for idx, ner in enumerate(self.ner):\n",
    "            ner[idx] = label2idx_dict[ner]\n",
    "    \n",
    "    def ner_idx2label(self, idx2label_dict):\n",
    "        if not isinstance(self.ner[0], int):\n",
    "            print(\"WARNING - NER not listed as indecies! Exiting...\")\n",
    "            return\n",
    "        for idx, ner in enumerate(self.ner):\n",
    "            ner[idx] = idx2label_dict[ner]\n",
    "\n",
    "class DataCollection:\n",
    "    def __init__(self, data_collection:list[DataItem], max_token_length=512):\n",
    "        self.max_token_length = max_token_length\n",
    "        self.data_collection:list[DataItem] = data_collection\n",
    "        self.unique_tags = self.get_unique_tags()\n",
    "        self.item_embeddings:dict = self.create_item_embeddings(self.unique_tags)\n",
    "        self.reverse_embeddings:dict = {v:k for k,v in self.item_embeddings.items()}\n",
    "\n",
    "    def get_token_list(self) -> list[list[str]]:\n",
    "        return [data_item.tokens for data_item in self.data_collection]\n",
    "\n",
    "    def get_pos_list(self) -> list[list[str]]:\n",
    "        return [data_item.pos for data_item in self.data_collection]\n",
    "\n",
    "    def get_ner_list(self) -> list[list[str]]:\n",
    "        return [data_item.ner for data_item in self.data_collection]\n",
    "    \n",
    "    def get_ner_idx_list(self) -> list[list[str]]:\n",
    "        ner_idx_list_collection = []\n",
    "        for data_item in self.data_collection:\n",
    "            ner_idx_list = []\n",
    "            for ner_tag in data_item.ner:\n",
    "                ner_idx_list.append(self.item_embeddings[ner_tag])\n",
    "            ner_idx_list_collection.append(ner_idx_list)\n",
    "        return ner_idx_list_collection\n",
    "\n",
    "    def get_unique_tags(self) -> list[str]:\n",
    "        unique_list = []\n",
    "        ner_tags_list:list = self.get_ner_list()\n",
    "        for ner_list in ner_tags_list:\n",
    "            for ner in ner_list:\n",
    "                if ner not in unique_list:\n",
    "                    unique_list.append(ner)\n",
    "        return unique_list\n",
    "    \n",
    "    def create_item_embeddings(self, tags:list[str]) -> dict:\n",
    "        return {label:idx for idx, label in enumerate(tags)}\n",
    "    \n",
    "    def get_invalid_token_lengths(self) -> list[int]:\n",
    "        invalid_lengths = []\n",
    "        for idx, data_item in enumerate(self.data_collection):\n",
    "            if len(data_item.tokenised_inputs[\"input_ids\"]) >= self.max_token_length:\n",
    "                invalid_lengths.append(idx)\n",
    "                print(\"Data item idx \",idx,\" has tokens longer than \",self.max_token_length)\n",
    "        return invalid_lengths\n",
    "\n",
    "    def remove_invalid_token_length_items(self) -> None:\n",
    "        invalid_lengths = self.get_invalid_token_lengths()\n",
    "        for index in sorted(invalid_lengths, reverse=True):\n",
    "            del self.data_collection[index]\n",
    "\n",
    "train_data:list[DataItem] = []\n",
    "for idx in range(len(train_tokens)):\n",
    "    train_data.append(DataItem(train_tokens[idx], train_pos_tags[idx], train_ner_tags[idx], idx))\n",
    "train_collection:DataCollection = DataCollection(train_data)\n",
    "\n",
    "validation_data:list[DataItem] = []\n",
    "for idx in range(len(validation_tokens)):\n",
    "    validation_data.append(DataItem(validation_tokens[idx], validation_pos_tags[idx], validation_ner_tags[idx], idx))\n",
    "validation_collection:DataCollection = DataCollection(validation_data)\n",
    "\n",
    "test_data:list[DataItem] = []\n",
    "for idx in range(len(test_tokens)):\n",
    "    test_data.append(DataItem(test_tokens[idx], test_pos_tags[idx], test_ner_tags[idx], idx))\n",
    "test_collection:DataCollection = DataCollection(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_collection.remove_invalid_token_length_items()\n",
    "validation_collection.remove_invalid_token_length_items()\n",
    "test_collection.remove_invalid_token_length_items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following https://huggingface.co/docs/transformers/main/en/tasks/token_classification\n",
    "\n",
    "# More on https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification.ipynb#scrollTo=IjyhFKjlEP_B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization Example\n",
    "\n",
    "Adds extra start and end tags (CLS and SEP), as well potentially splits one word into 2. Thus have to realign indecies.\n",
    "\n",
    "We also have to assign -100 to CLS and SEP so they are ignored by PyTorch loss function (CrossEntropyLoss)\n",
    "\n",
    "Only label first token of a word, add -100 for subtokens of the same word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'for', 'this', 'purpose', 'the', 'gothenburg', 'young', 'persons', 'empowerment', 'scale', '(', 'g', '##ype', '##s', ')', 'was', 'developed', '.', '[SEP]']\n",
      "{'input_ids': [101, 2005, 2023, 3800, 1996, 22836, 2402, 5381, 23011, 4094, 1006, 1043, 18863, 2015, 1007, 2001, 2764, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "example = dataset[\"train\"][0]\n",
    "tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "print(tokens)\n",
    "print(tokenized_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "def tokenize_and_align_labels(data_collection:DataCollection):\n",
    "    tokenized_inputs = tokenizer(data_collection.get_token_list(), truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(data_collection.get_ner_idx_list()):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_train = tokenize_and_align_labels(train_collection)\n",
    "tokenized_validation = tokenize_and_align_labels(validation_collection)\n",
    "tokenized_test = tokenize_and_align_labels(test_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = train_collection.unique_tags\n",
    "labels = train_collection.unique_tags\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "id2label = train_collection.reverse_embeddings\n",
    "label2id = train_collection.item_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(labels), id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_dict_to_list_of_dict(d):\n",
    "    new_list = []\n",
    "\n",
    "    for labels, inputs in zip(d[\"labels\"], d[\"input_ids\"]):\n",
    "        entry = {\"input_ids\": inputs, \"labels\": labels}\n",
    "        new_list.append(entry)\n",
    "\n",
    "    return new_list\n",
    "\n",
    "tokenised_train = turn_dict_to_list_of_dict(tokenized_train)\n",
    "tokenised_val = turn_dict_to_list_of_dict(tokenized_validation)\n",
    "tokenised_test = turn_dict_to_list_of_dict(tokenized_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olive\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\accelerate\\accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "559dc96065544e58bbd30c31a1f2d1f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10ebd41100df4b7bad984bc47e16ed0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory distilbert_model\\checkpoint-67 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5762450098991394, 'eval_precision': 0.872043918918919, 'eval_recall': 0.8261652330466093, 'eval_f1': 0.8484848484848484, 'eval_accuracy': 0.8296, 'eval_runtime': 0.1969, 'eval_samples_per_second': 777.179, 'eval_steps_per_second': 50.796, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8d23e35c99740d2904b3a7301a8bc31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7484994530677795, 'eval_precision': 0.8774989366227137, 'eval_recall': 0.8253650730146029, 'eval_f1': 0.8506339552623441, 'eval_accuracy': 0.823, 'eval_runtime': 0.1864, 'eval_samples_per_second': 821.002, 'eval_steps_per_second': 53.66, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6ba938d94e24c1ca6f9adbe8312e0ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8061400651931763, 'eval_precision': 0.8869174861051732, 'eval_recall': 0.8299659931986397, 'eval_f1': 0.8574971582101892, 'eval_accuracy': 0.8292, 'eval_runtime': 0.1813, 'eval_samples_per_second': 843.698, 'eval_steps_per_second': 55.144, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8e65d94ba084b54a9039e42fde8bd79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8855600953102112, 'eval_precision': 0.879940183721427, 'eval_recall': 0.8239647929585917, 'eval_f1': 0.8510330578512396, 'eval_accuracy': 0.8224, 'eval_runtime': 0.1756, 'eval_samples_per_second': 871.096, 'eval_steps_per_second': 56.934, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2aba4a90db04db3bda0122d289745e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.877755880355835, 'eval_precision': 0.887350936967632, 'eval_recall': 0.8335667133426685, 'eval_f1': 0.8596183599793709, 'eval_accuracy': 0.8324, 'eval_runtime': 0.1774, 'eval_samples_per_second': 862.404, 'eval_steps_per_second': 56.366, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a50304ea6d0b4696a47451eacc53a41d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9753626585006714, 'eval_precision': 0.886648270006368, 'eval_recall': 0.8355671134226845, 'eval_f1': 0.8603501544799176, 'eval_accuracy': 0.834, 'eval_runtime': 0.1795, 'eval_samples_per_second': 852.227, 'eval_steps_per_second': 55.701, 'epoch': 6.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3132f50f010f431eb9e1b0d08a3c64d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0449007749557495, 'eval_precision': 0.8811817597944765, 'eval_recall': 0.8233646729345869, 'eval_f1': 0.85129265770424, 'eval_accuracy': 0.8214, 'eval_runtime': 0.1999, 'eval_samples_per_second': 765.232, 'eval_steps_per_second': 50.015, 'epoch': 7.0}\n",
      "{'loss': 0.1602, 'learning_rate': 1.701492537313433e-05, 'epoch': 7.46}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14a1c0e1a4b345aeb84c413ff697aeca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0407954454421997, 'eval_precision': 0.8890308839190628, 'eval_recall': 0.8349669933986797, 'eval_f1': 0.8611512275634413, 'eval_accuracy': 0.8332, 'eval_runtime': 0.1804, 'eval_samples_per_second': 848.121, 'eval_steps_per_second': 55.433, 'epoch': 8.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f05b35585cc9412e93900972920c2ba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.073683261871338, 'eval_precision': 0.8860786397449522, 'eval_recall': 0.8339667933586717, 'eval_f1': 0.8592333058532563, 'eval_accuracy': 0.8324, 'eval_runtime': 0.1835, 'eval_samples_per_second': 833.665, 'eval_steps_per_second': 54.488, 'epoch': 9.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c0daaa31bc049b3bf4026c2fc5ea975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1222586631774902, 'eval_precision': 0.8875638841567292, 'eval_recall': 0.8337667533506702, 'eval_f1': 0.8598246518824137, 'eval_accuracy': 0.8318, 'eval_runtime': 0.1786, 'eval_samples_per_second': 856.647, 'eval_steps_per_second': 55.99, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7c1e962811e420cbba84c982b1873e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1650903224945068, 'eval_precision': 0.8838887709615793, 'eval_recall': 0.8329665933186637, 'eval_f1': 0.8576725025746652, 'eval_accuracy': 0.8316, 'eval_runtime': 0.1906, 'eval_samples_per_second': 802.525, 'eval_steps_per_second': 52.453, 'epoch': 11.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0057136751cc42dbbde6ebc239d688cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1740355491638184, 'eval_precision': 0.8847299021692897, 'eval_recall': 0.8321664332866573, 'eval_f1': 0.8576435419028966, 'eval_accuracy': 0.83, 'eval_runtime': 0.1816, 'eval_samples_per_second': 842.631, 'eval_steps_per_second': 55.074, 'epoch': 12.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "548e12bdfdc54ba6bc3880106b34b7e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2134870290756226, 'eval_precision': 0.884974533106961, 'eval_recall': 0.8341668333666733, 'eval_f1': 0.8588198949644734, 'eval_accuracy': 0.8322, 'eval_runtime': 0.1854, 'eval_samples_per_second': 825.069, 'eval_steps_per_second': 53.926, 'epoch': 13.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fe28061fbc84f2a8a07e5b9bb49bb03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2318637371063232, 'eval_precision': 0.8857569933803118, 'eval_recall': 0.8297659531906382, 'eval_f1': 0.8568477587275356, 'eval_accuracy': 0.8284, 'eval_runtime': 0.19, 'eval_samples_per_second': 805.159, 'eval_steps_per_second': 52.625, 'epoch': 14.0}\n",
      "{'loss': 0.0209, 'learning_rate': 1.4029850746268658e-05, 'epoch': 14.93}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "983e26eed44f4f769382d9fbe4db8a6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2416068315505981, 'eval_precision': 0.8864313058273076, 'eval_recall': 0.8337667533506702, 'eval_f1': 0.8592928564065561, 'eval_accuracy': 0.831, 'eval_runtime': 0.1936, 'eval_samples_per_second': 790.091, 'eval_steps_per_second': 51.64, 'epoch': 15.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa3d25a51d1442faa50302647593e3cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.278630256652832, 'eval_precision': 0.8848536637470626, 'eval_recall': 0.8285657131426285, 'eval_f1': 0.8557851239669423, 'eval_accuracy': 0.8268, 'eval_runtime': 0.181, 'eval_samples_per_second': 845.077, 'eval_steps_per_second': 55.234, 'epoch': 16.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa6926d966df4e39a7dfabf4d6d7e21d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.273123025894165, 'eval_precision': 0.8851796725494365, 'eval_recall': 0.8327665533106622, 'eval_f1': 0.8581735724592868, 'eval_accuracy': 0.8308, 'eval_runtime': 0.1832, 'eval_samples_per_second': 835.254, 'eval_steps_per_second': 54.592, 'epoch': 17.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bd283ca86b345f3af826e1bb9b8ef88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3050628900527954, 'eval_precision': 0.8875689435723377, 'eval_recall': 0.8369673934786958, 'eval_f1': 0.8615257901781119, 'eval_accuracy': 0.8342, 'eval_runtime': 0.1837, 'eval_samples_per_second': 832.74, 'eval_steps_per_second': 54.427, 'epoch': 18.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d2bb341a4c2426996eca1e27bf33b04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.317600965499878, 'eval_precision': 0.8857384680490902, 'eval_recall': 0.837367473494699, 'eval_f1': 0.8608740359897172, 'eval_accuracy': 0.8354, 'eval_runtime': 0.1921, 'eval_samples_per_second': 796.541, 'eval_steps_per_second': 52.062, 'epoch': 19.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22bc37237d064c4c8d7e8887ae218f6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.347732424736023, 'eval_precision': 0.8874707509040629, 'eval_recall': 0.8345669133826765, 'eval_f1': 0.8602061855670103, 'eval_accuracy': 0.8316, 'eval_runtime': 0.1847, 'eval_samples_per_second': 828.251, 'eval_steps_per_second': 54.134, 'epoch': 20.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5860b50f31d3498984046d982b69bd0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3546124696731567, 'eval_precision': 0.8855075547988934, 'eval_recall': 0.832366473294659, 'eval_f1': 0.8581150752732521, 'eval_accuracy': 0.83, 'eval_runtime': 0.2056, 'eval_samples_per_second': 744.008, 'eval_steps_per_second': 48.628, 'epoch': 21.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e143dc59b78743b986d5864ebc08a722",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3885867595672607, 'eval_precision': 0.8871413390010627, 'eval_recall': 0.8349669933986797, 'eval_f1': 0.8602638087386645, 'eval_accuracy': 0.8316, 'eval_runtime': 0.1896, 'eval_samples_per_second': 806.973, 'eval_steps_per_second': 52.743, 'epoch': 22.0}\n",
      "{'loss': 0.0057, 'learning_rate': 1.1044776119402986e-05, 'epoch': 22.39}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a26f79ec80e14e5f8c7f16578233efc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3998149633407593, 'eval_precision': 0.8864313058273076, 'eval_recall': 0.8337667533506702, 'eval_f1': 0.8592928564065561, 'eval_accuracy': 0.831, 'eval_runtime': 0.1938, 'eval_samples_per_second': 789.331, 'eval_steps_per_second': 51.59, 'epoch': 23.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e1b3d554365407f950d4a01aef91263",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4089477062225342, 'eval_precision': 0.8851623859053279, 'eval_recall': 0.8341668333666733, 'eval_f1': 0.858908341915551, 'eval_accuracy': 0.831, 'eval_runtime': 0.2054, 'eval_samples_per_second': 745.067, 'eval_steps_per_second': 48.697, 'epoch': 24.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f4e58cdcd604ce0a0911720bfbd5334",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4438952207565308, 'eval_precision': 0.8841723549488054, 'eval_recall': 0.8291658331666333, 'eval_f1': 0.855786105089295, 'eval_accuracy': 0.827, 'eval_runtime': 0.1947, 'eval_samples_per_second': 785.939, 'eval_steps_per_second': 51.369, 'epoch': 25.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35068450e25f4313a4bff2c5d45d61dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4539906978607178, 'eval_precision': 0.8846317581949766, 'eval_recall': 0.831366273254651, 'eval_f1': 0.8571723213364959, 'eval_accuracy': 0.8282, 'eval_runtime': 0.1958, 'eval_samples_per_second': 781.241, 'eval_steps_per_second': 51.062, 'epoch': 26.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0739673dbc6149d1ac0ec7dc1fafb916",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4531972408294678, 'eval_precision': 0.8865429729153338, 'eval_recall': 0.8315663132626525, 'eval_f1': 0.8581750619322874, 'eval_accuracy': 0.8292, 'eval_runtime': 0.1917, 'eval_samples_per_second': 798.01, 'eval_steps_per_second': 52.158, 'epoch': 27.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90013e4683d44fbead4d013e6a8d0874",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4620660543441772, 'eval_precision': 0.8850746268656716, 'eval_recall': 0.830366073214643, 'eval_f1': 0.8568479719269273, 'eval_accuracy': 0.828, 'eval_runtime': 0.1873, 'eval_samples_per_second': 816.839, 'eval_steps_per_second': 53.388, 'epoch': 28.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "476f0db8d4b5486791e6cd71bc0f793b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.49458646774292, 'eval_precision': 0.8837855159154027, 'eval_recall': 0.8275655131026205, 'eval_f1': 0.8547520661157025, 'eval_accuracy': 0.8256, 'eval_runtime': 0.2, 'eval_samples_per_second': 764.926, 'eval_steps_per_second': 49.995, 'epoch': 29.0}\n",
      "{'loss': 0.0026, 'learning_rate': 8.059701492537314e-06, 'epoch': 29.85}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26000a98729a40b48183c543ff9ad1e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.490684151649475, 'eval_precision': 0.8845580404685836, 'eval_recall': 0.8307661532306462, 'eval_f1': 0.8568186507117804, 'eval_accuracy': 0.8288, 'eval_runtime': 0.197, 'eval_samples_per_second': 776.508, 'eval_steps_per_second': 50.752, 'epoch': 30.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17e4ae1878f9426fbb6721205fc5f93d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.478588342666626, 'eval_precision': 0.8858116744780571, 'eval_recall': 0.8317663532706542, 'eval_f1': 0.8579387186629527, 'eval_accuracy': 0.8294, 'eval_runtime': 0.2035, 'eval_samples_per_second': 751.881, 'eval_steps_per_second': 49.143, 'epoch': 31.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56a9b821c62f49a7be25fbb81615ef5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4939507246017456, 'eval_precision': 0.8842217484008529, 'eval_recall': 0.8295659131826365, 'eval_f1': 0.8560222933223244, 'eval_accuracy': 0.827, 'eval_runtime': 0.1937, 'eval_samples_per_second': 789.726, 'eval_steps_per_second': 51.616, 'epoch': 32.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11e236860e2446808ea05903ae2c430a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4894871711730957, 'eval_precision': 0.8864023870417732, 'eval_recall': 0.8319663932786557, 'eval_f1': 0.858322154576411, 'eval_accuracy': 0.829, 'eval_runtime': 0.2057, 'eval_samples_per_second': 743.733, 'eval_steps_per_second': 48.61, 'epoch': 33.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "232cd18bc8f04c6dabbf6963f2a4bde8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5210912227630615, 'eval_precision': 0.8850991259859305, 'eval_recall': 0.8305661132226445, 'eval_f1': 0.8569659442724458, 'eval_accuracy': 0.8274, 'eval_runtime': 0.1936, 'eval_samples_per_second': 790.42, 'eval_steps_per_second': 51.661, 'epoch': 34.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6f65f7f030145b4899475b883e792f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5031026601791382, 'eval_precision': 0.8863587997446265, 'eval_recall': 0.8331666333266653, 'eval_f1': 0.8589399876263146, 'eval_accuracy': 0.8302, 'eval_runtime': 0.1937, 'eval_samples_per_second': 789.789, 'eval_steps_per_second': 51.62, 'epoch': 35.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84124cd4c3364f98a40381d277677785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5021260976791382, 'eval_precision': 0.8870658427445131, 'eval_recall': 0.8327665533106622, 'eval_f1': 0.8590590177465952, 'eval_accuracy': 0.8308, 'eval_runtime': 0.2015, 'eval_samples_per_second': 759.473, 'eval_steps_per_second': 49.639, 'epoch': 36.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "768549e2464d4b68b5ea1fb138341e16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.507655382156372, 'eval_precision': 0.887137989778535, 'eval_recall': 0.833366673334667, 'eval_f1': 0.859412068076328, 'eval_accuracy': 0.8312, 'eval_runtime': 0.2, 'eval_samples_per_second': 765.11, 'eval_steps_per_second': 50.007, 'epoch': 37.0}\n",
      "{'loss': 0.0016, 'learning_rate': 5.074626865671642e-06, 'epoch': 37.31}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0a0f85df9694d1b9c776a3675f669c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5143983364105225, 'eval_precision': 0.8869250425894378, 'eval_recall': 0.8331666333266653, 'eval_f1': 0.8592057761732852, 'eval_accuracy': 0.831, 'eval_runtime': 0.1937, 'eval_samples_per_second': 789.697, 'eval_steps_per_second': 51.614, 'epoch': 38.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad42ea79d300445f8d2dd81ed4cea647",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5133357048034668, 'eval_precision': 0.885835995740149, 'eval_recall': 0.8319663932786557, 'eval_f1': 0.858056529812255, 'eval_accuracy': 0.8306, 'eval_runtime': 0.2045, 'eval_samples_per_second': 748.22, 'eval_steps_per_second': 48.903, 'epoch': 39.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "381f27776fce4112aae013f991baa56e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.517109990119934, 'eval_precision': 0.886615515771526, 'eval_recall': 0.8321664332866573, 'eval_f1': 0.858528531627283, 'eval_accuracy': 0.8304, 'eval_runtime': 0.2, 'eval_samples_per_second': 764.844, 'eval_steps_per_second': 49.99, 'epoch': 40.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d0cf1f150204a31869d3cd75902d6b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5299570560455322, 'eval_precision': 0.8853543979504697, 'eval_recall': 0.8295659131826365, 'eval_f1': 0.856552721264071, 'eval_accuracy': 0.8278, 'eval_runtime': 0.1975, 'eval_samples_per_second': 774.705, 'eval_steps_per_second': 50.634, 'epoch': 41.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bcfa3f1bd66430dba9801178aca944c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5329430103302002, 'eval_precision': 0.8862862010221465, 'eval_recall': 0.8325665133026605, 'eval_f1': 0.8585869004641569, 'eval_accuracy': 0.8302, 'eval_runtime': 0.1857, 'eval_samples_per_second': 824.033, 'eval_steps_per_second': 53.858, 'epoch': 42.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6d938859541470899ba0674f53f6285",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5291471481323242, 'eval_precision': 0.8866396761133604, 'eval_recall': 0.832366473294659, 'eval_f1': 0.8586463062319438, 'eval_accuracy': 0.8304, 'eval_runtime': 0.202, 'eval_samples_per_second': 757.349, 'eval_steps_per_second': 49.5, 'epoch': 43.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3e238d901db494d9ca22951cec0c163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5256043672561646, 'eval_precision': 0.8866879659211928, 'eval_recall': 0.8327665533106622, 'eval_f1': 0.8588817825459047, 'eval_accuracy': 0.831, 'eval_runtime': 0.1987, 'eval_samples_per_second': 770.015, 'eval_steps_per_second': 50.328, 'epoch': 44.0}\n",
      "{'loss': 0.001, 'learning_rate': 2.08955223880597e-06, 'epoch': 44.78}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d9e3294f25d4e279e695aea383ee70f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5262418985366821, 'eval_precision': 0.8869250425894378, 'eval_recall': 0.8331666333266653, 'eval_f1': 0.8592057761732852, 'eval_accuracy': 0.8314, 'eval_runtime': 0.1936, 'eval_samples_per_second': 790.132, 'eval_steps_per_second': 51.643, 'epoch': 45.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5427d6e316054c7badeb0ea32468c130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5400372743606567, 'eval_precision': 0.886378170965679, 'eval_recall': 0.8317663532706542, 'eval_f1': 0.8582043343653252, 'eval_accuracy': 0.83, 'eval_runtime': 0.2081, 'eval_samples_per_second': 735.059, 'eval_steps_per_second': 48.043, 'epoch': 46.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7552b4beb1744f57a3984582da61a68d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5360428094863892, 'eval_precision': 0.8864507882403068, 'eval_recall': 0.832366473294659, 'eval_f1': 0.8585577220674714, 'eval_accuracy': 0.8306, 'eval_runtime': 0.1963, 'eval_samples_per_second': 779.587, 'eval_steps_per_second': 50.953, 'epoch': 47.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a4c832d605a4dcda76e679d2cf3d563",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5318161249160767, 'eval_precision': 0.8860732538330494, 'eval_recall': 0.832366473294659, 'eval_f1': 0.858380608561114, 'eval_accuracy': 0.8308, 'eval_runtime': 0.1961, 'eval_samples_per_second': 780.356, 'eval_steps_per_second': 51.004, 'epoch': 48.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b1e4be4b70648e6a15851bcef899d90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5407421588897705, 'eval_precision': 0.8860004261666311, 'eval_recall': 0.8317663532706542, 'eval_f1': 0.8580272389599669, 'eval_accuracy': 0.83, 'eval_runtime': 0.2007, 'eval_samples_per_second': 762.168, 'eval_steps_per_second': 49.815, 'epoch': 49.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be25c1dcaf534ca5ae57626b4d210204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5382275581359863, 'eval_precision': 0.8864507882403068, 'eval_recall': 0.832366473294659, 'eval_f1': 0.8585577220674714, 'eval_accuracy': 0.8308, 'eval_runtime': 0.2138, 'eval_samples_per_second': 715.614, 'eval_steps_per_second': 46.772, 'epoch': 50.0}\n",
      "{'train_runtime': 213.2798, 'train_samples_per_second': 251.079, 'train_steps_per_second': 15.707, 'train_loss': 0.028731520594055975, 'epoch': 50.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3350, training_loss=0.028731520594055975, metrics={'train_runtime': 213.2798, 'train_samples_per_second': 251.079, 'train_steps_per_second': 15.707, 'train_loss': 0.028731520594055975, 'epoch': 50.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output_dir = \"distilbert_model\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_output_dir,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3, # number of epochs to train\n",
    "    weight_decay=0.01, # The weight decay to apply (if not zero) to all layers except all bias and LayerNorm weights\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\", # can save by epoch, steps or not at all\n",
    "    save_total_limit=1, # how many checkpoints to keep before overriding (set to 1, so latest checkpoint is only kept)!\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=['none'], # REQUIRED because otherwise keeps asking to log into \"wandb\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenised_train,\n",
    "    eval_dataset=tokenised_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83de24d4ef6c4458b809ebd2cf8f422a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5762450098991394,\n",
       " 'eval_precision': 0.872043918918919,\n",
       " 'eval_recall': 0.8261652330466093,\n",
       " 'eval_f1': 0.8484848484848484,\n",
       " 'eval_accuracy': 0.8296,\n",
       " 'eval_runtime': 0.1827,\n",
       " 'eval_samples_per_second': 837.596,\n",
       " 'eval_steps_per_second': 54.745,\n",
       " 'epoch': 50.0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac124eee79cc478ab5209506bd92bbc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'AC': {'precision': 0.7269372693726938,\n",
       "  'recall': 0.7490494296577946,\n",
       "  'f1': 0.7378277153558053,\n",
       "  'number': 263},\n",
       " 'LF': {'precision': 0.4508670520231214,\n",
       "  'recall': 0.5131578947368421,\n",
       "  'f1': 0.48000000000000004,\n",
       "  'number': 152},\n",
       " 'O': {'precision': 0.9624645892351275,\n",
       "  'recall': 0.9568176484393335,\n",
       "  'f1': 0.9596328115805578,\n",
       "  'number': 4261},\n",
       " 'overall_precision': 0.9299145299145299,\n",
       " 'overall_recall': 0.93071000855432,\n",
       " 'overall_f1': 0.9303120991876871,\n",
       " 'overall_accuracy': 0.9224}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Prepare the test data for evaluation in the same format as the training data\n",
    "\n",
    "# predictions, labels, _ = trainer.predict(tokenised_test)\n",
    "# predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# # label_list = test_collection.get_ner_idx_list()\n",
    "# label_list = train_collection.unique_tags\n",
    "\n",
    "# # Remove the predictions for the [CLS] and [SEP] tokens \n",
    "# true_predictions = [\n",
    "#     [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "#     for prediction, label in zip(predictions, labels)\n",
    "# ]\n",
    "# true_labels = [\n",
    "#     [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "#     for prediction, label in zip(predictions, labels)\n",
    "# ]\n",
    "\n",
    "# # Compute multiple metrics on the test restuls\n",
    "# results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "# results\n",
    "predictions, labels, _ = trainer.predict(tokenised_val) # tokenized validation used instead of validation dataset (as recommended for vectorised)\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# Remove ignored index (special tokens)\n",
    "true_predictions = [\n",
    "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "true_labels = [\n",
    "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "\n",
    "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n",
      "['=', 'manual', 'ability', 'classification', 'system', ';', 'quest', '=', 'quest', '-', 'quality', 'of', 'upper', 'extremity', 'skills', 'test', ';', 'cont', '=', 'control', ';', 'm', '=', 'male', ',', 'f', '=', 'female', ',', 'v', '=', 'verbal', ',', 'nonv', '=', 'non', '-', 'verbal', ',', '|quad', '=', 'quadriplegia', ',', 'di', '=', 'diplegia', ',', 'hemi', '=', 'hemiplegia', '.']\n",
      "67\n",
      "{'input_ids': [101, 1027, 6410, 3754, 5579, 2291, 1025, 8795, 1027, 8795, 1011, 3737, 1997, 3356, 4654, 7913, 16383, 4813, 3231, 1025, 9530, 2102, 1027, 2491, 1025, 1049, 1027, 3287, 1010, 1042, 1027, 2931, 1010, 1058, 1027, 12064, 1010, 2512, 2615, 1027, 2512, 1011, 12064, 1010, 1064, 17718, 1027, 17718, 29443, 23115, 2401, 1010, 4487, 1027, 16510, 23115, 2401, 1010, 19610, 2072, 1027, 19610, 11514, 23115, 2401, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "67\n",
      "['[CLS]', '=', 'manual', 'ability', 'classification', 'system', ';', 'quest', '=', 'quest', '-', 'quality', 'of', 'upper', 'ex', '##tre', '##mity', 'skills', 'test', ';', 'con', '##t', '=', 'control', ';', 'm', '=', 'male', ',', 'f', '=', 'female', ',', 'v', '=', 'verbal', ',', 'non', '##v', '=', 'non', '-', 'verbal', ',', '|', 'quad', '=', 'quad', '##rip', '##leg', '##ia', ',', 'di', '=', 'dip', '##leg', '##ia', ',', 'hem', '##i', '=', 'hem', '##ip', '##leg', '##ia', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "for data_item in validation_collection.data_collection:\n",
    "    t = data_item.tokens\n",
    "    print(len(t))\n",
    "    print(t)\n",
    "    tok = tokenizer(t, is_split_into_words=True)\n",
    "    print(len(tok[\"input_ids\"]))\n",
    "    print(tok) # tokenizer adds the CLS and SEP! So should be safe to rip out first and last character\n",
    "    toks = []\n",
    "    for to in tok[\"input_ids\"]:\n",
    "        toks.append(tokenizer.convert_ids_to_tokens(to))\n",
    "    print(len(toks))\n",
    "    print(toks)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100, 0, 1, 2, 2, 2, 0, 3, 0, 1, 2, 2, 2, 2, 2, -100, -100, 2, 2, 0, 0, -100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, 0, 0, 0, 0, 0, 0, -100, 0, 0, -100, -100, -100, 0, 0, 0, 0, -100, -100, 0, 0, -100, 0, 0, -100, -100, -100, 0, -100]\n",
      "['', 'B-O', 'B-LF', 'I-LF', 'I-LF', 'I-LF', 'B-O', 'B-AC', 'B-O', 'B-LF', 'I-LF', 'I-LF', 'I-LF', 'I-LF', 'I-LF', '', '', 'I-LF', 'I-LF', 'B-O', 'B-O', '', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', '', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', '', 'B-O', 'B-O', '', '', '', 'B-O', 'B-O', 'B-O', 'B-O', '', '', 'B-O', 'B-O', '', 'B-O', 'B-O', '', '', '', 'B-O', '']\n",
      "67\n"
     ]
    }
   ],
   "source": [
    "id2label[-100] = \"\"\n",
    "validation_labels =  [label for label in tokenized_validation[\"labels\"][0]]\n",
    "print(validation_labels)\n",
    "validation_label_ids = [id2label[label] for label in tokenized_validation[\"labels\"][0]]\n",
    "print(validation_label_ids)\n",
    "print(len(validation_label_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentance: electro - oculography ( eog ) ( retiport32 , roland consult , wiesbaden , germany ) was performed in all patients according to the guidelines of the international society for clinical electrophysiology of vision ( iscev).[12 ] arden ratios below 1.8 were rated as pathologic .\n",
      "token length: 46\n",
      "original length: 71\n",
      "['electro', '-', 'o', '##cu', '##log', '##raphy', '(', 'e', '##og', ')', '(', 're', '##tip', '##ort', '##32', ',', 'roland', 'consult', ',', 'wi', '##es', '##bad', '##en', ',', 'germany', ')', 'was', 'performed', 'in', 'all', 'patients', 'according', 'to', 'the', 'guidelines', 'of', 'the', 'international', 'society', 'for', 'clinical', 'electro', '##phy', '##sio', '##logy', 'of', 'vision', '(', 'is', '##ce', '##v', ')', '.', '[', '12', ']', 'arden', 'ratios', 'below', '1', '.', '8', 'were', 'rated', 'as', 'path', '##olo', '##gic', '.']\n",
      "after removing cls and sep length: 69\n",
      "predicted tokens length: 69\n",
      "['B-LF', 'I-LF', 'I-LF', 'I-LF', 'I-LF', 'I-LF', 'B-O', 'B-AC', 'B-AC', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'I-LF', 'I-LF', 'I-LF', 'I-LF', 'I-LF', 'I-LF', 'I-LF', 'I-LF', 'I-LF', 'B-O', 'B-AC', 'B-AC', 'B-AC', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O']\n"
     ]
    }
   ],
   "source": [
    "idx_num = 1\n",
    "sentances = []\n",
    "for data_item in validation_collection.data_collection:\n",
    "    sentances.append({\"sentance\":\" \".join(data_item.tokens),\"token_len\":len(data_item.tokens)})\n",
    "print(\"sentance:\", sentances[idx_num][\"sentance\"])\n",
    "print(\"token length:\", sentances[idx_num][\"token_len\"])\n",
    "\n",
    "tokenized_input = [data.tokenised_inputs for data in validation_collection.data_collection]\n",
    "print(\"original length:\", len(tokenized_input[idx_num][\"input_ids\"]))\n",
    "token_values = []\n",
    "for token in tokenized_input[idx_num][\"input_ids\"]:\n",
    "    token_value = tokenizer.convert_ids_to_tokens(token)\n",
    "    if token_value != \"[CLS]\" and token_value != \"[SEP]\":\n",
    "        token_values.append(token_value)\n",
    "print(token_values)\n",
    "print(\"after removing cls and sep length:\", len(token_values))\n",
    "\n",
    "checkpoint_list:list[str] = os.listdir(model_output_dir)\n",
    "last_checkpoint:str = checkpoint_list[-1:][0]\n",
    "last_checkpoint_path:str = os.path.join(model_output_dir, last_checkpoint)\n",
    "classifier = pipeline(\"ner\", model=last_checkpoint_path)\n",
    "result:list[dict] = classifier(sentances[idx_num][\"sentance\"])\n",
    "print(\"predicted tokens length:\",len(result))\n",
    "print([value[\"entity\"] for value in result])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentances = []\n",
    "for data_item in validation_collection.data_collection:\n",
    "    sentances.append({\"sentance\":\" \".join(data_item.tokens),\"token_len\":len(data_item.tokens)})\n",
    "\n",
    "measure_dict = []\n",
    "id2label = validation_collection.reverse_embeddings\n",
    "tokenized_validation\n",
    "for idx, tokenised in enumerate(tokenised_val):\n",
    "    # d = {\"tags\":[], \"sentance\":\"\",\"classifier_res\":[]}\n",
    "    tokenised_l = tokenised[\"labels\"][1:-1]\n",
    "    tags = []\n",
    "    prev_id = list(validation_collection.item_embeddings.keys())[0]\n",
    "    for label_id in tokenised_l:\n",
    "        if label_id != -100:\n",
    "            tags.append(id2label[label_id])\n",
    "            prev_id = id2label[label_id]\n",
    "        else:\n",
    "            tags.append(prev_id)\n",
    "    tokenised_in = tokenised[\"input_ids\"][1:-1]\n",
    "    tokens = []\n",
    "    for token in tokenised_in:\n",
    "        tokens.append(tokenizer.convert_ids_to_tokens(token))\n",
    "    result:list[dict] = classifier(sentances[idx][\"sentance\"])\n",
    "    classifier_entities = [value[\"entity\"] for value in result]\n",
    "    measure_dict.append({\"tags\":tags, \"tokens\":tokens, \"sentance\":sentances[idx][\"sentance\"], \"predicted_tags\": classifier_entities})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t -snares syn-1a and snap25 through their interactions with pm - bound voltage - gated calcium channels ( cav ) , l - type in Î² - cells and n - type in neurons , position the predocked sgs to the site of maximum ca2 + influx for efficient exocytosis [ 6â€“12 ] .\n",
      "71\n",
      "71\n"
     ]
    }
   ],
   "source": [
    "idx_val = 3\n",
    "print(measure_dict[idx_val][\"sentance\"])\n",
    "print(len(measure_dict[idx_val][\"tags\"]))\n",
    "print(len(measure_dict[idx_val][\"predicted_tags\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION COLLECTION SIZE:  126\n",
      "TOTAL TOKENS IN VALIDATION SET: 6549\n",
      "TOTAL VALUES: {'B-O': {'correct': 4969, 'incorrect': 239, 'total': 5208}, 'B-LF': {'correct': 56, 'incorrect': 233, 'total': 289}, 'I-LF': {'correct': 410, 'incorrect': 79, 'total': 489}, 'B-AC': {'correct': 324, 'incorrect': 239, 'total': 563}}\n"
     ]
    }
   ],
   "source": [
    "tags = validation_collection.item_embeddings.keys()\n",
    "\n",
    "class TagMetrics:\n",
    "    def __init__(self, tag:str, tag_dict:dict):\n",
    "        self.tag = tag\n",
    "        self.correct = tag_dict[\"correct\"]\n",
    "        self.incorrect = tag_dict[\"incorrect\"]\n",
    "        self.total = tag_dict[\"total\"]\n",
    "\n",
    "class MetricItem:\n",
    "    def __init__(self, total:int, correct:int, incorrect:int, tags_dict:dict, idx=0):\n",
    "        self.idx:int = idx\n",
    "        self.total:int = total\n",
    "        self.correct:int = correct\n",
    "        self.incorrect:int = incorrect\n",
    "        self.tag_metric:list[TagMetrics] = [TagMetrics(tag, tags_dict[tag]) for tag in tags_dict.keys()]\n",
    "\n",
    "class MetricCollection:\n",
    "    def __init__(self, metric_items:list[MetricItem]):\n",
    "        self.data_collection:list[MetricItem] = metric_items\n",
    "        self.total_correct:int = sum([item.correct for item in metric_items]) \n",
    "        self.total_incorrect:int = sum([item.incorrect for item in metric_items])\n",
    "        self.total_label_measurement:dict = self.__items_to_label_measure__()\n",
    "\n",
    "    def __items_to_label_measure__(self) -> dict:\n",
    "        tag_measure = {}\n",
    "        for item in metric_items:\n",
    "            for tag_metric in item.tag_metric:\n",
    "                if tag_metric.tag not in tag_measure.keys():\n",
    "                    tag_measure[tag_metric.tag] = {\"correct\":tag_metric.correct, \"incorrect\":tag_metric.incorrect, \"total\":tag_metric.total}\n",
    "                else:\n",
    "                    tag_measure[tag_metric.tag][\"correct\"] += tag_metric.correct\n",
    "                    tag_measure[tag_metric.tag][\"incorrect\"] += tag_metric.incorrect\n",
    "                    tag_measure[tag_metric.tag][\"total\"] += tag_metric.total\n",
    "        return tag_measure\n",
    "\n",
    "print(\"VALIDATION COLLECTION SIZE: \", len(validation_collection.data_collection))\n",
    "total_tokens = 0\n",
    "for idx in range(len(measure_dict)):\n",
    "    total_tokens += len(measure_dict[idx][\"tokens\"])\n",
    "print(\"TOTAL TOKENS IN VALIDATION SET:\", total_tokens)\n",
    "\n",
    "\n",
    "metric_items:list[MetricItem] = []\n",
    "for idx in range(len(measure_dict)):\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "    tags_dict = {tag:{\"correct\":0, \"incorrect\":0, \"total\":0} for tag in tags}\n",
    "    current_total = len(measure_dict[idx][\"tokens\"])\n",
    "    for d_idx in range(current_total):\n",
    "        tag = measure_dict[idx][\"tags\"][d_idx]\n",
    "        predicted_tag = measure_dict[idx][\"predicted_tags\"][d_idx]\n",
    "        if tag == predicted_tag:\n",
    "            correct += 1\n",
    "            tags_dict[tag][\"correct\"] += 1\n",
    "            tags_dict[tag][\"total\"] += 1\n",
    "        else:\n",
    "            incorrect += 1\n",
    "            tags_dict[tag][\"incorrect\"] += 1\n",
    "            tags_dict[tag][\"total\"] += 1\n",
    "    metric_items.append(MetricItem(current_total, correct, incorrect, tags_dict, idx=idx))\n",
    "    # break # for only 1 item\n",
    "\n",
    "metric_collection:MetricCollection = MetricCollection(metric_items)\n",
    "print(\"TOTAL VALUES:\", metric_collection.total_label_measurement)\n",
    "# print(\"IDX:1\")\n",
    "# print(\"TOTAL CORRECT:\",correct,\"/\",current_total)\n",
    "# print(\"TOTAL INCORRECT:\",incorrect,\"/\",current_total)\n",
    "# for k in tags_dict.keys():\n",
    "#     print(k, \":\", tags_dict[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       B-O  B-LF  I-LF  B-AC\n",
      "B-O   4969    19   132    88\n",
      "B-LF   104    56   126     3\n",
      "I-LF    77     1   410     1\n",
      "B-AC   232     0     7   324\n",
      "Precision: 0.652353426919901\n",
      "Recall: 0.6816220880069025\n",
      "F1_Score: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "tags = validation_collection.get_unique_tags()\n",
    "df = pd.DataFrame(0, columns=tags, index=tags)\n",
    "\n",
    "for idx in range(len(measure_dict)):\n",
    "    for d_idx in range(len(measure_dict[idx][\"tokens\"])):\n",
    "        tag = measure_dict[idx][\"tags\"][d_idx]\n",
    "        predicted_tag = measure_dict[idx][\"predicted_tags\"][d_idx]\n",
    "        df.at[tag, predicted_tag] += 1 # first is row, second is column (meaning rows are true tags and columns are predicted tags)\n",
    "print(df) # center diagonally is TP (and TN) and everything else is FP (and FN)!\n",
    "\n",
    "def calc_precision(TP, FP):\n",
    "    return TP / (TP + FP)\n",
    "\n",
    "def calc_recall(TP, FN):\n",
    "    return TP / (TP + FN)\n",
    "\n",
    "def calc_f1(precision, recall):\n",
    "    return (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "total_correct = 0\n",
    "for tag in tags:\n",
    "    total_correct += df.at[tag,tag]\n",
    "\n",
    "total_incorrect = total_tokens - total_correct\n",
    "\n",
    "TP = 56 + 410 + 324\n",
    "TN = 4969\n",
    "FP = 104 + 1 + 7 + 77 + 0 + 232\n",
    "FN = 19 + 126 + 1 + 132 + 3 + 88\n",
    "# precision = calc_precision(total_correct, total_incorrect)\n",
    "# recall = calc_recall(total_correct, total_incorrect)\n",
    "# f1_score = calc_f1(precision, recall)\n",
    "\n",
    "precision = calc_precision(TP, FP)\n",
    "recall = calc_recall(TP, FN)\n",
    "f1_score = calc_f1(precision, recall)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1_Score:\", f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Predicted Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>=</td>\n",
       "      <td>B-O</td>\n",
       "      <td>B-O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>manual</td>\n",
       "      <td>B-LF</td>\n",
       "      <td>B-LF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ability</td>\n",
       "      <td>I-LF</td>\n",
       "      <td>I-LF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>classification</td>\n",
       "      <td>I-LF</td>\n",
       "      <td>I-LF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>system</td>\n",
       "      <td>I-LF</td>\n",
       "      <td>I-LF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>hem</td>\n",
       "      <td>B-O</td>\n",
       "      <td>B-O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>##ip</td>\n",
       "      <td>B-O</td>\n",
       "      <td>B-O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>##leg</td>\n",
       "      <td>B-O</td>\n",
       "      <td>B-O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>##ia</td>\n",
       "      <td>B-O</td>\n",
       "      <td>B-O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>.</td>\n",
       "      <td>B-O</td>\n",
       "      <td>B-O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Tokens  Tags Predicted Tags\n",
       "0                =   B-O            B-O\n",
       "1           manual  B-LF           B-LF\n",
       "2          ability  I-LF           I-LF\n",
       "3   classification  I-LF           I-LF\n",
       "4           system  I-LF           I-LF\n",
       "..             ...   ...            ...\n",
       "60             hem   B-O            B-O\n",
       "61            ##ip   B-O            B-O\n",
       "62           ##leg   B-O            B-O\n",
       "63            ##ia   B-O            B-O\n",
       "64               .   B-O            B-O\n",
       "\n",
       "[65 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataframe the first item in validation set!\n",
    "\n",
    "data = []\n",
    "for idx in range(len(measure_dict)):\n",
    "    for d_idx in range(len(measure_dict[idx][\"tokens\"])):\n",
    "        data.append([measure_dict[idx][\"tokens\"][d_idx], measure_dict[idx][\"tags\"][d_idx], measure_dict[idx][\"predicted_tags\"][d_idx]])\n",
    "    break\n",
    "df = pd.DataFrame(data, columns=['Tokens', 'Tags', \"Predicted Tags\"])\n",
    "df.to_csv('result.csv', index=False)  \n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
