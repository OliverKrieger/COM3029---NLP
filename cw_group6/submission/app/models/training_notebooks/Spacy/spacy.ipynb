{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrain_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict, Dataset # Import dataset import function for hugging face\n",
    "dataset_dict:DatasetDict = load_dataset(\"surrey-nlp/PLOD-CW\") # import the coursework dataset from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "train_dict = dataset_dict[\"train\"]\n",
    "test_dict = dataset_dict[\"test\"]\n",
    "validation_dict = dataset_dict[\"validation\"]\n",
    "\n",
    "train_tokens = [row[\"tokens\"] for row in train_dict]\n",
    "train_pos_tags = [row[\"ner_tags\"] for row in train_dict]\n",
    "train_ner_tags = [row[\"ner_tags\"] for row in train_dict]\n",
    "\n",
    "validation_tokens = [row[\"tokens\"] for row in validation_dict]\n",
    "validation_pos_tags = [row[\"ner_tags\"] for row in validation_dict]\n",
    "validation_ner_tags = [row[\"ner_tags\"] for row in validation_dict]\n",
    "\n",
    "test_tokens = [row[\"tokens\"] for row in test_dict]\n",
    "test_pos_tags = [row[\"ner_tags\"] for row in test_dict]\n",
    "test_ner_tags = [row[\"ner_tags\"] for row in test_dict]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_list(given_list:list[list[any]]) -> list[any]:\n",
    "    return [element for inner_list in given_list for element in inner_list]\n",
    "\n",
    "def data_to_lower(data:list[list[str]]) -> list[list[str]]:\n",
    "    return [[token.lower() for token in tokens] for tokens in data]\n",
    "\n",
    "train_tokens = data_to_lower(train_tokens)\n",
    "validation_tokens = data_to_lower(validation_tokens)\n",
    "test_tokens = data_to_lower(test_tokens)\n",
    "\n",
    "class DataItem:\n",
    "    def __init__(self, tokens, pos, ner, idx=0):\n",
    "        self.idx=idx\n",
    "        self.tokens:list[str] = tokens\n",
    "        self.pos:list[str] = pos\n",
    "        self.ner:list = ner\n",
    "\n",
    "class DataCollection:\n",
    "    def __init__(self, data_collection:list[DataItem], max_token_length=512):\n",
    "        self.max_token_length = max_token_length\n",
    "        self.data_collection:list[DataItem] = data_collection\n",
    "        self.unique_tags = self.get_unique_tags()\n",
    "        self.item_embeddings:dict = self.create_item_embeddings(self.unique_tags)\n",
    "        self.reverse_embeddings:dict = {v:k for k,v in self.item_embeddings.items()}\n",
    "\n",
    "    def get_token_list(self) -> list[list[str]]:\n",
    "        return [data_item.tokens for data_item in self.data_collection]\n",
    "\n",
    "    def get_pos_list(self) -> list[list[str]]:\n",
    "        return [data_item.pos for data_item in self.data_collection]\n",
    "\n",
    "    def get_ner_list(self) -> list[list[str]]:\n",
    "        return [data_item.ner for data_item in self.data_collection]\n",
    "    \n",
    "    def get_ner_idx_list(self) -> list[list[str]]:\n",
    "        ner_idx_list_collection = []\n",
    "        for data_item in self.data_collection:\n",
    "            ner_idx_list = []\n",
    "            for ner_tag in data_item.ner:\n",
    "                ner_idx_list.append(self.item_embeddings[ner_tag])\n",
    "            ner_idx_list_collection.append(ner_idx_list)\n",
    "        return ner_idx_list_collection\n",
    "\n",
    "    \n",
    "    def get_unique_tags(self) -> list[str]:\n",
    "        return list(set(flatten_list(self.get_ner_list())))\n",
    "    \n",
    "    def create_item_embeddings(self, tags:list[str]) -> dict:\n",
    "        return {label:idx for idx, label in enumerate(tags)}\n",
    "    \n",
    "    def get_tag_count(self) -> dict:\n",
    "        tag_dict = {}\n",
    "        for tag_list in self.get_ner_list():\n",
    "            for tag in tag_list:\n",
    "                if tag not in tag_dict.keys():\n",
    "                    tag_dict[tag] = 1\n",
    "                else:\n",
    "                    tag_dict[tag] += 1\n",
    "        return tag_dict\n",
    "\n",
    "def dataset_to_collection(dataset) -> DataCollection:\n",
    "    data_items:list[DataItem] = []\n",
    "    for idx in range(len(dataset)):\n",
    "        data_items.append(DataItem(dataset[\"tokens\"][idx], dataset[\"pos_tags\"][idx], dataset[\"ner_tags\"][idx], idx))\n",
    "    return DataCollection(data_items)\n",
    "\n",
    "train_data:list[DataItem] = []\n",
    "for idx in range(len(train_tokens)):\n",
    "    train_data.append(DataItem(train_tokens[idx], train_pos_tags[idx], train_ner_tags[idx], idx))\n",
    "train_collection:DataCollection = DataCollection(train_data)\n",
    "\n",
    "validation_data:list[DataItem] = []\n",
    "for idx in range(len(validation_tokens)):\n",
    "    validation_data.append(DataItem(validation_tokens[idx], validation_pos_tags[idx], validation_ner_tags[idx], idx))\n",
    "validation_collection:DataCollection = DataCollection(validation_data)\n",
    "\n",
    "test_data:list[DataItem] = []\n",
    "for idx in range(len(test_tokens)):\n",
    "    test_data.append(DataItem(test_tokens[idx], test_pos_tags[idx], test_ner_tags[idx], idx))\n",
    "test_collection:DataCollection = DataCollection(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string  \n",
    "\n",
    "# spacy\n",
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Doc, DocBin\n",
    "nlp:Language = spacy.blank(\"en\")\n",
    "\n",
    "# configuration\n",
    "def create_dir(dir_path):\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "\n",
    "working_dir:str = os.getcwd()\n",
    "vocab_dir:str = os.path.join(working_dir, \"spacy_vocab\")\n",
    "create_dir(vocab_dir)\n",
    "\n",
    "config_dir:str = os.path.join(working_dir, \"config\")\n",
    "create_dir(config_dir)\n",
    "\n",
    "output_dir:str = os.path.join(working_dir, \"output\")\n",
    "create_dir(output_dir)\n",
    "\n",
    "train_vocab_path = os.path.join(vocab_dir, \"train.spacy\")\n",
    "dev_vocab_path = os.path.join(vocab_dir, \"dev.spacy\")\n",
    "config_path = os.path.join(config_dir, \"config.cfg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docbin:DocBin = DocBin()\n",
    "test_docbin:DocBin = DocBin()\n",
    "\n",
    "def dataset_to_vocab(collection:DataCollection, doc_bin:DocBin) -> dict:\n",
    "    for data_item in collection.data_collection:\n",
    "        spaces = [True if token not in string.punctuation else False for token in data_item.tokens] \n",
    "        doc = Doc(nlp.vocab, words=data_item.tokens, spaces=spaces, ents=data_item.ner)\n",
    "        doc_bin.add(doc)\n",
    "\n",
    "dataset_to_vocab(train_collection, train_docbin)\n",
    "dataset_to_vocab(test_collection, test_docbin)\n",
    "\n",
    "train_docbin.to_disk(train_vocab_path)\n",
    "test_docbin.to_disk(dev_vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Saving to output directory:\n",
      "c:\\Users\\olive\\OneDrive\\Documents\\Uni\\TEST - FYP -\n",
      "SpacyEntityFinder_V3\\spacy\\output\u001b[0m\n",
      "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olive\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['transformer', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
      "E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  -------------  --------  ------  ------  ------  ------\n",
      "  0       0        1644.56   1143.95    6.87    5.69    8.66    0.07\n",
      " 40     200       60542.23  64085.80   87.25   86.79   87.73    0.87\n",
      " 80     400        1498.42   1508.59   86.49   86.33   86.64    0.86\n",
      "120     600         235.75    258.09   85.20   85.20   85.20    0.85\n",
      "160     800         605.29    468.50   86.64   86.64   86.64    0.87\n",
      "200    1000         105.87    104.10   86.49   86.33   86.64    0.86\n",
      "240    1200          34.14     29.90   86.13   85.97   86.28    0.86\n",
      "280    1400          74.09     57.51   86.23   86.55   85.92    0.86\n",
      "320    1600          43.12     33.95   87.00   87.00   87.00    0.87\n",
      "360    1800          59.35     46.71   85.10   84.64   85.56    0.85\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "c:\\Users\\olive\\OneDrive\\Documents\\Uni\\TEST - FYP -\n",
      "SpacyEntityFinder_V3\\spacy\\output\\model-last\n",
      "Training finished with time:  466.55s\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "spacy_train_time = \"\"\n",
    "if retrain_model:\n",
    "    from spacy.cli.train import train\n",
    "    training_start_time = time.time()\n",
    "    train(config_path=config_path, output_path=output_dir, overrides={\"paths.train\": train_vocab_path, \"paths.dev\": dev_vocab_path}, use_gpu=0)\n",
    "    spacy_train_time = '{:.2f}s'.format(time.time() - training_start_time)\n",
    "print(\"Training finished with time: \", spacy_train_time)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
